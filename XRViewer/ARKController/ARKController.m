#import "ARKController.h"
#import <os/lock.h>
#import "WebARKHeader.h"
#import <AVFoundation/AVFoundation.h>
#import "ARKSceneKitController.h"
#import "ARKMetalController.h"
#import "HitAnchor.h"
#import "HitTestResult.h"
#import "Utils.h"
#import "XRViewer-Swift.h"
#import <Accelerate/Accelerate.h>
#import "Constants.h"
#import "Compression.h"
#import <JavaScriptCore/JavaScriptCore.h>

const float CAMERA_FRAME_SCALE_FACTOR = 0.4; //0.295;
const float CAMERA_FRAME_JPEG_COMPRESSION_FACTOR = 0.1;

double lastConvertTime = 0;

@interface ARKController () <ARSessionDelegate>
{
    NSDictionary *arkData;
    os_unfair_lock lock;
    NSMutableDictionary *objects; // key - JS anchor name : value - ARAnchor NSUUID string
    NSDictionary* computerVisionData;
}

@property (nonatomic, strong) id<ARKControllerProtocol> controller;

@property (nonatomic, copy) NSDictionary *request;
@property (nonatomic, strong) ARSession *session;

@property (nonatomic, strong) ARConfiguration *configuration;

@property (nonatomic, strong) AVCaptureDevice *device;

@property(nonatomic) ShowMode showMode;
@property(nonatomic) ShowOptions showOptions;

@property(nonatomic, strong) ARWorldMap *backgroundWorldMap;

/*
 Computer vision properties
 We hold different data structures, like accelerate, NSData, and NSString buffers,
 to avoid allocating/deallocating a huge amount of memory on each frame
 */
/// Luma buffer
@property vImage_Buffer lumaBuffer;
/// A temporary luma buffer used by the Accelerate framework in the buffer scale opration
@property void* lumaScaleTemporaryBuffer;
/// The luma buffer size that's being sent to JS
@property CGSize lumaBufferSize;
/// A data buffer holding the luma information. It's created only onced reused on every frame
/// by means of the replaceBytesInRange method
@property(nonatomic, strong) NSMutableData* lumaDataBuffer;
/// The luma string buffer being sent to JS
@property(nonatomic, strong) NSMutableString* lumaBase64StringBuffer;
/*
 The same properties for luma are used for chroma
 */
@property vImage_Buffer chromaBuffer;
@property void* chromaScaleTemporaryBuffer;
@property CGSize chromaBufferSize;
@property(nonatomic, strong) NSMutableData* chromaDataBuffer;
@property(nonatomic, strong) NSMutableString* chromaBase64StringBuffer;

/// The CV image being sent to JS is downscaled using the metho
/// downscaleByFactorOf2UntilLargestSideIsLessThan512AvoidingFractionalSides
/// This call has a side effect on computerVisionImageScaleFactor, that's later used
/// in order to scale the intrinsics of the camera
@property (nonatomic) float computerVisionImageScaleFactor;

/// Dictionary holding ARReferenceImages by name
@property(nonatomic, strong) NSMutableDictionary* referenceImageMap;
/// Dictionary holding completion blocks by image name
@property(nonatomic, strong) NSMutableDictionary* detectionImageActivationPromises;
/// Dictionary holding completion blocks by image name
@property(nonatomic, strong) NSMutableDictionary* detectionImageCreationPromises;
/// Array holding dictionaries representing detection image data
@property(nonatomic, strong) NSMutableArray *detectionImageCreationRequests;
/// Dictionary holding completion blocks by image name: when an image anchor is removed,
/// if the name exsist in this dictionary, call activate again using the callback stored here.
@property(nonatomic, strong) NSMutableDictionary* detectionImageActivationAfterRemovalPromises;

/// completion block for getWorldMap request
@property(nonatomic, strong) GetWorldMapCompletionBlock getWorldMapPromise;
@property(nonatomic, strong) SetWorldMapCompletionBlock setWorldMapPromise;

// for saving
@property(nonatomic, strong) NSURL *worldSaveURL;

/**
 We don't send the face geometry on every frame, for performance reasons. This number indicates the
 current number of frames without sending the face geometry
 */
@property int numberOfFramesWithoutSendingFaceGeometry;
@end

@implementation ARKController {
    /// Array of anchor dictionaries that were added since the last frame.
    /// Contains the initial data of the anchor when it was added.
    NSMutableArray *addedAnchorsSinceLastFrame;

    /// Array of anchor IDs that were removed since the last frame
    NSMutableArray *removedAnchorsSinceLastFrame;

    /// Dictionary that maps a user-generated anchor ID with the one generated by ARKit
    NSMutableDictionary *arkitGeneratedAnchorIDUserAnchorIDMap;
}

#pragma mark Interface

- (void)dealloc
{
    DDLogDebug(@"ARKController dealloc");
}

- (instancetype)initWithType:(ARKType)type rootView:(UIView *)rootView
{
    self = [super init];
    
    if (self)
    {
        lock = OS_UNFAIR_LOCK_INIT;
        objects = [NSMutableDictionary new];
        addedAnchorsSinceLastFrame = [NSMutableArray new];
        removedAnchorsSinceLastFrame = [NSMutableArray new];
        arkitGeneratedAnchorIDUserAnchorIDMap = [NSMutableDictionary new];
        [self setShouldUpdateWindowSize:YES];

        [self setSession:[ARSession new]];
        [[self session] setDelegate:self];
        [self setArSessionState:ARKSessionUnknown];
        
        // don't want anyone using this
        self.backgroundWorldMap = nil;

        /**
         A configuration for running world tracking.
         
         @discussion World tracking provides 6 degrees of freedom tracking of the device.
         By finding feature points in the scene, world tracking enables performing hit-tests against the frame.
         Tracking can no longer be resumed once the session is paused.
         */
        
        ARWorldTrackingConfiguration* worldTrackingConfiguration = [ARWorldTrackingConfiguration new];
        
        [worldTrackingConfiguration setPlaneDetection:ARPlaneDetectionHorizontal | ARPlaneDetectionVertical];
        [worldTrackingConfiguration setWorldAlignment:ARWorldAlignmentGravityAndHeading];
        [self setConfiguration: worldTrackingConfiguration];
        
        Class cls = (type == ARKMetal) ? [ARKMetalController class] : [ARKSceneKitController class];
        id<ARKControllerProtocol> controller = [[cls alloc] initWithSesion:[self session] size:[rootView bounds].size];
        [self setController:controller];
        [rootView addSubview:[controller renderView]];
        [[controller renderView] setTranslatesAutoresizingMaskIntoConstraints:NO];
        [[[[controller renderView] topAnchor] constraintEqualToAnchor:[rootView topAnchor]] setActive:YES];
        [[[[controller renderView] leftAnchor] constraintEqualToAnchor:[rootView leftAnchor]] setActive:YES];
        [[[[controller renderView] rightAnchor] constraintEqualToAnchor:[rootView rightAnchor]] setActive:YES];
        [[[[controller renderView] bottomAnchor] constraintEqualToAnchor:[rootView bottomAnchor]] setActive:YES];
        
        [[self controller] setHitTestFocusPoint:[[[self controller] renderView] center]];

        self.interfaceOrientation = [Utils getInterfaceOrientationFromDeviceOrientation];
        
        self.lumaDataBuffer = nil;
        self.lumaBase64StringBuffer = nil;
        self.chromaDataBuffer = nil;
        self.chromaBase64StringBuffer = nil;
        self.computerVisionImageScaleFactor = 4.0;
        self.lumaBufferSize = CGSizeMake(0.0f, 0.0f);

        self.sendingWorldSensingDataAuthorizationStatus = SendWorldSensingDataAuthorizationStateNotDetermined;
        self.detectionImageActivationPromises = [NSMutableDictionary new];
        self.referenceImageMap = [NSMutableDictionary new];
        self.detectionImageCreationRequests = [NSMutableArray new];
        self.detectionImageCreationPromises = [NSMutableDictionary new];
        self.detectionImageActivationAfterRemovalPromises = [NSMutableDictionary new];
        
        self.getWorldMapPromise = nil;
        self.setWorldMapPromise = nil;
        
        NSFileManager *filemgr = [NSFileManager defaultManager];

        NSArray *dirPaths = NSSearchPathForDirectoriesInDomains(NSDocumentDirectory,
                                                       NSUserDomainMask, YES );
        NSURL *docsDir = [NSURL fileURLWithPath:[dirPaths objectAtIndex:0]];
        NSURL *newDir = [docsDir URLByAppendingPathComponent:@"maps" isDirectory:YES];
        //if ([storeURL checkResourceIsReachableAndReturnError:&error]) {
        NSError* theError = nil;
        if ([filemgr createDirectoryAtURL:newDir withIntermediateDirectories:YES attributes:nil error:&theError] == NO)
        {
            // Failed to create directory
            self.worldSaveURL = nil;
            DDLogError(@"Couldn't create map save directory error - %@", theError);
        } else {
            self.worldSaveURL = [newDir URLByAppendingPathComponent:@"webxrviewer"];
        }
 
        self.numberOfFramesWithoutSendingFaceGeometry = 0;
    }
    
    return self;
}

- (void)setupDeviceCamera
{
    [self setDevice:[AVCaptureDevice defaultDeviceWithMediaType:AVMediaTypeVideo]];
    
    if ([self device] == nil)
    {
        DDLogError(@"Camera device is NIL");
        return;
    }
    
    NSError *outError;
    [[self device] lockForConfiguration:&outError];
    
    if ([[self device] lockForConfiguration:&outError])
    {
        if ([[self device] isFocusModeSupported:AVCaptureFocusModeContinuousAutoFocus])
        {
            DDLogDebug(@"AVCaptureFocusModeContinuousAutoFocus Supported");
            [[self device] setFocusMode:AVCaptureFocusModeContinuousAutoFocus];
        }
        
        if ([[self device] isFocusPointOfInterestSupported])
        {
            DDLogDebug(@"FocusPointOfInterest Supported");
            [[self device] setFocusPointOfInterest:CGPointMake(0.5, 0.5)];
        }
        
        if ([[self device] isSmoothAutoFocusSupported])
        {
            DDLogDebug(@"SmoothAutoFocus Supported");
            [[self device] setSmoothAutoFocusEnabled:YES];
        }
        
        [[self device] unlockForConfiguration];
    }
    else
    {
        DDLogError(@"Camera lock error - %@", outError);
    }
}

- (void)viewWillTransitionToSize:(CGSize)size
{
    [[self controller] setHitTestFocusPoint:CGPointMake(size.width / 2, size.height / 2)];
    self.interfaceOrientation = [Utils getInterfaceOrientationFromDeviceOrientation];
}

- (void)pauseSession
{
    [[self session] pause];
    [self setArSessionState:ARKSessionPaused];
}

- (NSDictionary *)arkData
{
//    NSDictionary *data;
    
//    os_unfair_lock_lock(&(lock));
//    data = arkData;
//    os_unfair_lock_unlock(&(lock));
    
//    return [data copy];
    return arkData;
}

- (NSDictionary*)computerVisionData {
//    NSDictionary* data;
    
//    os_unfair_lock_lock(&(lock));
//    data = computerVisionData;
//    os_unfair_lock_unlock(&(lock));
    
//    return [data copy];
    return computerVisionData;
}

- (NSTimeInterval)currentFrameTimeInMilliseconds {
    return self.session.currentFrame.timestamp * 1000;
}


- (void)saveWorldMap {
    if (![self trackingStateNormal]) {
        DDLogError(@"can't save WorldMap to local storage until tracking is initialized");
        return;
    }
    
    if (![self worldMappingAvailable]) {
        DDLogError(@"can't save WorldMap to local storage until World Mapping has started");
        return;
    }

    [[self session] getCurrentWorldMapWithCompletionHandler:^(ARWorldMap * _Nullable worldMap, NSError * _Nullable error) {
        if (worldMap) {
            DDLogError(@"saving WorldMap to local storage");
            [self _saveWorldMap:worldMap];
        }
    }];
}

- (void)_saveWorldMap:(ARWorldMap *)worldMap {
    if (self.worldSaveURL) {
        if (!worldMap) {
            // try to get rid of an old one if it exists.  Don't care if this fails.
            [[NSFileManager defaultManager] trashItemAtURL:self.worldSaveURL resultingItemURL:nil error:nil];
            DDLogError(@"moving saved WorldMap to trash");
        } else {
            NSData * data     = [NSKeyedArchiver archivedDataWithRootObject: worldMap requiringSecureCoding:YES error:nil];
            if ([data writeToURL:self.worldSaveURL atomically:YES] == NO) {
                DDLogError(@"Failed saving WorldMap to persistent storage");
            } else {
                DDLogError(@"saved WorldMap to load storage at %@", self.worldSaveURL);
            }
        }
    }
}

- (void)loadSavedMap {
    if (self.worldSaveURL) {
        NSData * data = [NSData dataWithContentsOfURL:self.worldSaveURL];
        if (!data) {
            DDLogError(@"Failed to load saved WorldMap from persistent storage");
            return;
        }

        ARWorldMap* obj = [NSKeyedUnarchiver unarchivedObjectOfClass:[ARWorldMap class]  fromData:data error:nil];
        if (!obj) {
            DDLogError(@"Failed to create ARWorldMap from saved WorldMap loaded from persistent storage");
        }
        [self _setWorldMap:obj];
    }
}

// this is the web command to save and return the world map
- (void)getWorldMap:(GetWorldMapCompletionBlock)completion {
    if (self.getWorldMapPromise) {
        self.getWorldMapPromise(NO, @"World Map request cancelled by subsequent call to get World Map.", nil);
        self.getWorldMapPromise = nil;
    }
    
#ifdef ALLOW_GET_WORLDMAP
    switch (self.sendingWorldSensingDataAuthorizationStatus) {
        case SendWorldSensingDataAuthorizationStateAuthorized: {
            self.getWorldMapPromise = completion;
            [self _getWorldMap];
            break;
        }
        case SendWorldSensingDataAuthorizationStateDenied: {
            completion(NO, @"The user denied access to world sensing data", nil);
            break;
        }
        case SendWorldSensingDataAuthorizationStateNotDetermined: {
            NSLog(@"Attempt to get World Map but world sensing data authorization is not determined, enqueue the request");
            self.getWorldMapPromise = completion;
            break;
        }
    }
#else
    completion(NO, @"getWorldMap not supported", nil);
#endif
}

- (NSData *) getDecompressedData:(NSData *) compressed {
    size_t dst_buffer_size = compressed.length * 8;
    
    uint8_t *src_buffer = malloc(compressed.length);
    [compressed getBytes:src_buffer length:compressed.length];

    while (YES) {
        uint8_t *dst_buffer = malloc(dst_buffer_size);
        size_t decompressedSize = compression_decode_buffer(dst_buffer, dst_buffer_size, src_buffer, compressed.length, nil, COMPRESSION_ZLIB);

        // error!
        if (decompressedSize == 0) {
            free(dst_buffer);
            free(src_buffer);
            return NULL;
        }

        // overflow, try again
        if (decompressedSize == dst_buffer_size) {
            dst_buffer_size *= 2;
            free(dst_buffer);
            continue;
        }
        NSData *decompressed = [[NSData alloc] initWithBytes:dst_buffer length:decompressedSize];
        free(dst_buffer);
        free(src_buffer);
        return decompressed;
    }
}

- (NSData *) getCompressedData:(NSData*) input {
    size_t dst_buffer_size = MAX(input.length / 8, 10);

    uint8_t *src_buffer = malloc(input.length);
    [input getBytes:src_buffer length:input.length];

    while (YES)
    {
        uint8_t *dst_buffer = malloc(dst_buffer_size);
        size_t compressedSize = compression_encode_buffer(dst_buffer, dst_buffer_size, src_buffer, input.length, nil, COMPRESSION_ZLIB);

        // overflow, try again
        if (compressedSize == 0) {
            dst_buffer_size *= 2;
            free(dst_buffer);
            continue;
        }
        NSData *compressed = [[NSData alloc] initWithBytes:dst_buffer length:compressedSize];
        free(dst_buffer);
        free(src_buffer);
        return compressed;
    }
}

- (void)_printWorldMapInfo:(ARWorldMap*) worldMap {
    NSArray<ARAnchor *> *anchors = worldMap.anchors;
    for (ARAnchor* anchor in anchors) {
        NSString *anchorID;
        if ([anchor isKindOfClass:[ARPlaneAnchor class]]) {
            // ARKit system plane anchor;  probably shouldn't happen!
            anchorID = [anchor.identifier UUIDString];
            DDLogWarn(@"saved WorldMap: contained PlaneAnchor");
        } else if ([anchor isKindOfClass:[ARImageAnchor class]]) {
            // User generated ARImageAnchor;  probably shouldn't happen!
            ARImageAnchor *imageAnchor = (ARImageAnchor *)anchor;
            anchorID = imageAnchor.referenceImage.name;
            DDLogWarn(@"saved WorldMap: contained trackable ImageAnchor");
        } else if ([anchor isKindOfClass:[ARFaceAnchor class]]) {
            // System generated ARFaceAnchor;  probably shouldn't happen!
            anchorID = [anchor.identifier UUIDString];
            DDLogWarn(@"saved WorldMap: contained trackable FaceAnchor");
        } else {
            anchorID = anchor.name;
        }
        NSLog(@"WorldMap contains anchor: %@", anchorID);
    }
    simd_float3 center = worldMap.center;
    simd_float3 extent = worldMap.extent;
    NSLog(@"Map center: %g, %g, %g", center[0], center[1], center[2]);
    NSLog(@"Map extent: %g, %g, %g", extent[0], extent[1], extent[2]);
}

// actually do the saving and sending of world map back to the app
- (void)_getWorldMap {
    GetWorldMapCompletionBlock completion = self.getWorldMapPromise;
    self.getWorldMapPromise = nil;
    
    if ([[self configuration] isKindOfClass:[ARFaceTrackingConfiguration class]]) {
        if (completion) {
            completion(NO, @"Cannot get World Map when using the front facing camera", nil);
        }
        return;
    }

    if (![self trackingStateNormal]) {
        if (completion) {
            completion(NO, @"Cannot get World Map until tracking is fully initialized", nil);
        }
        return;
    }

    if (![self worldMappingAvailable]) {
        if (completion) {
            completion(NO, @"Cannot get World Map until World Mapping has started", nil);
        }
        return;
    }

    [[self session] getCurrentWorldMapWithCompletionHandler:^(ARWorldMap * _Nullable worldMap, NSError * _Nullable error) {
        if (worldMap) {
            if (completion) {
                NSMutableDictionary *mapData = [NSMutableDictionary new];

                NSData * data     = [NSKeyedArchiver archivedDataWithRootObject: worldMap requiringSecureCoding:YES error:nil];
                NSData * compressedData = [self getCompressedData:data];

                if (!compressedData) {
                    completion(NO, @"request to get World Map failed: couldn't compress data", nil);
                    return;
                }
                NSLog(@"world map uncompressed size %lu -> compressed %lu", (unsigned long)data.length, (unsigned long)compressedData.length);

                NSString * string = [compressedData base64EncodedStringWithOptions:0];
                mapData[@"worldMap"] = string;

                NSArray<ARAnchor *> *anchors = worldMap.anchors;
                NSMutableArray *anchorList = [NSMutableArray arrayWithCapacity:anchors.count];
                for (ARAnchor* anchor in anchors) {
                    // include any anchor with a name in the list, since they've likely been
                    // created by the web app
                    if (anchor.name) {
                        [anchorList addObject:anchor.name];
                    }
                }
                mapData[@"anchors"] = anchorList;
                mapData[@"center"] = dictFromVector3(worldMap.center);
                mapData[@"extent"] = dictFromVector3(worldMap.extent);
                mapData[@"featureCount"] = @(worldMap.rawFeaturePoints.count);
                
                [self _printWorldMapInfo:worldMap];

                completion(YES, nil, mapData);
                DDLogError(@"saving WorldMap due to web request");
            }
        } else {
            completion(NO, [NSString stringWithFormat:@"request to get World Map failed: %@", error], nil);
        }
    }];
}

- (ARWorldMap *)dictToWorldMap:(NSDictionary*)worldMapDictionary {
    NSString* b64String = worldMapDictionary[@"worldMap"];
    if (!b64String) {
        return nil;
    }
    NSData *data    = [[NSData alloc] initWithBase64EncodedString:b64String options:(NSDataBase64DecodingIgnoreUnknownCharacters)];
    if (!data) {
        return nil;
    }
    NSData *uncompressed = [self getDecompressedData:data];
    NSLog(@"world map compressed size %lu -> uncompressed %lu", (unsigned long)data.length, (unsigned long)uncompressed.length);
    ARWorldMap* obj = [NSKeyedUnarchiver unarchivedObjectOfClass:[ARWorldMap class]  fromData:uncompressed error:nil];
    
    return obj;
}

- (void)setWorldMap:(NSDictionary *)worldMapDictionary completion:(SetWorldMapCompletionBlock)completion {
    if (self.setWorldMapPromise) {
        self.setWorldMapPromise(NO, @"World Map set request cancelled by subsequent call to set World Map.");
    }
    
    switch (self.sendingWorldSensingDataAuthorizationStatus) {
        case SendWorldSensingDataAuthorizationStateAuthorized: {
            self.setWorldMapPromise = completion;
            // we do it here (rather than above) because if a nil is passed in, we don't want to replace the previously saved
            // value (since a user could still reload it with the browser menu)
            ARWorldMap* map = [self dictToWorldMap : worldMapDictionary];
            
            if (map) {
                //[self _saveWorldMap:map];
                [self _setWorldMap: map];
            } else {
                if (self.setWorldMapPromise) {
                    self.setWorldMapPromise(NO, @"The World Map may be invalid, it couldn't be decoded.");
                    self.setWorldMapPromise = nil;
                }
            }
            break;
        }
        case SendWorldSensingDataAuthorizationStateDenied: {
            completion(NO, @"The user denied access to world sensing data, so cannot set map");
            break;
        }
        case SendWorldSensingDataAuthorizationStateNotDetermined: {
            NSLog(@"Attempt to get World Map but world sensing data authorization is not determined, enqueue the request");
            self.setWorldMapPromise = completion;
            break;
        }
    }
}

- (void)_setWorldMap:(ARWorldMap *)map  {
    SetWorldMapCompletionBlock completion = self.setWorldMapPromise;
    self.setWorldMapPromise = nil;

    if (map == nil) {
        DDLogError(@"nil WorldMap");
        if (completion) {
            completion(NO, @"nil World Map, this error shouldn't happen..");
        }
        return;
    }

    if ([[self configuration] isKindOfClass:[ARWorldTrackingConfiguration class]]) {
        // first, let's restart with curret configuration, but remove existing anchors
    //    [[self session] runWithConfiguration:[self configuration] options: ARSessionRunOptionRemoveExistingAnchors];
        
        NSLog(@"Restarted, removing existing anchors");

        // now, let's load the world map
        ARWorldTrackingConfiguration* worldTrackingConfiguration = (ARWorldTrackingConfiguration*)[self configuration];
        [worldTrackingConfiguration setInitialWorldMap:map];

        [self _printWorldMapInfo:map];

        [[self session] runWithConfiguration:[self configuration] options: ARSessionRunOptionResetTracking | ARSessionRunOptionRemoveExistingAnchors];

        NSLog(@"Restarted, loading map.");

        NSArray<ARAnchor *> *anchors = map.anchors;
        for (ARAnchor* anchor in anchors) {
            [[self session] addAnchor:anchor];
            arkitGeneratedAnchorIDUserAnchorIDMap[[[anchor identifier] UUIDString]] = anchor.name;
            NSLog(@"WorldMap loaded anchor: %@", anchor.name);
        }
        
        // now remove the map from the config
        [worldTrackingConfiguration setInitialWorldMap:nil];

        [self setArSessionState:ARKSessionRunning];
        // [self setupDeviceCamera];

        if (completion) {
            completion(YES, nil);
        }
        DDLogError(@"using Saved WorldMap to restart session");
    } else {
        DDLogError(@"Cannot load World Map when using user-facing camera");
        if (completion) {
            completion(NO, @"Cannot load World Map when using user-facing camera");
        }
        return;
    }
}


- (BOOL) hasBackgroundWorldMap {
    return (self.backgroundWorldMap != nil);
}

- (void)saveWorldMapInBackground {
    if (![self trackingStateNormal]) {
        DDLogError(@"can't save WorldMap as we transition to background, tracking isn't initialized");
        return;
    }
    
    [[self session] getCurrentWorldMapWithCompletionHandler:^(ARWorldMap * _Nullable worldMap, NSError * _Nullable error) {
        if (worldMap) {
            DDLogError(@"saving WorldMap as we transition to background");
            self.backgroundWorldMap = worldMap;
        }
    }];
}

// the session was paused, which implies it was off of the AR page, somewhere 2D, for a bit
- (void)resumeSessionWithAppState: (AppState*)state {
    [self setRequest:[state aRRequest]];
    
    if ([[self configuration] isKindOfClass:[ARWorldTrackingConfiguration class]]) {
        ARWorldTrackingConfiguration* worldTrackingConfiguration = (ARWorldTrackingConfiguration*)[self configuration];
        if ([self hasBackgroundWorldMap]) {
            [worldTrackingConfiguration setInitialWorldMap:[self backgroundWorldMap]];
            self.backgroundWorldMap = nil;
            DDLogError(@"using Saved WorldMap to resume session");
        } else {
            [worldTrackingConfiguration setInitialWorldMap:nil];
            DDLogError(@"no Saved WorldMap, resuming without background worldmap");
        }
    } else {
        DDLogError(@"resume session on a face-tracking camera");
    }
    [[self session] runWithConfiguration:[self configuration]];
    [self setArSessionState:ARKSessionRunning];
    [self setupDeviceCamera];
    
    [self setShowMode:[state showMode]];
    [self setShowOptions:[state showOptions]];
}

// the app was backgrounded, so try to reactivate the session map
- (void)resumeSessionFromBackground: (AppState*)state {
    [self setRequest:[state aRRequest]];
    
    if ([[self configuration] isKindOfClass:[ARWorldTrackingConfiguration class]]) {
        ARWorldTrackingConfiguration* worldTrackingConfiguration = (ARWorldTrackingConfiguration*)[self configuration];
        if ([self hasBackgroundWorldMap]) {
            [worldTrackingConfiguration setInitialWorldMap:[self backgroundWorldMap]];
            self.backgroundWorldMap = nil;
            DDLogError(@"using Saved WorldMap to resume session");
        } else {
            [worldTrackingConfiguration setInitialWorldMap:nil];
            DDLogError(@"no Saved WorldMap, resuming without background worldmap");
        }
    } else {
        DDLogError(@"resume session on a face-tracking camera");
    }
    [[self session] runWithConfiguration:[self configuration]];
    [self setArSessionState:ARKSessionRunning];
}


- (void)startSessionWithAppState:(AppState *)state
{
    [self updateARConfigurationWithState:state];
    [[self session] runWithConfiguration:[self configuration] options: ARSessionRunOptionResetTracking | ARSessionRunOptionRemoveExistingAnchors];
    [self setArSessionState:ARKSessionRunning];
    
    // if we've already received authorization for CV or WorldState date, likely because of a preference setting or
    // previous saved approval for the site, make sure we set up the state properly here
    if ([state askedComputerVisionData]) {
        [self setComputerVisionDataEnabled:[state userGrantedSendingComputerVisionData]];
    }
    if ([state askedWorldStateData]) {
        [self setSendingWorldSensingDataAuthorizationStatus:[state userGrantedSendingWorldStateData] ? SendWorldSensingDataAuthorizationStateAuthorized: SendWorldSensingDataAuthorizationStateDenied];
    }
    
    [self setupDeviceCamera];
    
    [self setShowMode:[state showMode]];
    [self setShowOptions:[state showOptions]];
}

- (void)runSessionRemovingAnchorsWithAppState:(AppState *)state {
    [self updateARConfigurationWithState:state];
    [[self session] runWithConfiguration:[self configuration] options: ARSessionRunOptionRemoveExistingAnchors];
    [self setArSessionState:ARKSessionRunning];
}

- (void)updateARConfigurationWithState:(AppState *)state {
    [self setRequest:[state aRRequest]];

    // make sure there is no initial worldmap set
    if ([[self configuration] isKindOfClass:[ARWorldTrackingConfiguration class]]) {
        ARWorldTrackingConfiguration* worldTrackingConfiguration = (ARWorldTrackingConfiguration*)[self configuration];
        [worldTrackingConfiguration setInitialWorldMap:nil];
        if ([self hasBackgroundWorldMap]) {
            self.backgroundWorldMap = nil;
            DDLogError(@"clearing Saved Background WorldMap from resume session");
        }
    }
    
    if ([[state aRRequest][WEB_AR_WORLD_ALIGNMENT] boolValue]) {
        [[self configuration] setWorldAlignment:ARWorldAlignmentGravityAndHeading];
    } else {
        [[self configuration] setWorldAlignment:ARWorldAlignmentGravity];
    }
}

- (void)runSessionResettingTrackingAndRemovingAnchorsWithAppState:(AppState *)state {
    [self updateARConfigurationWithState:state];
    [[self session] runWithConfiguration:[self configuration] options:ARSessionRunOptionResetTracking | ARSessionRunOptionRemoveExistingAnchors];
    [self setArSessionState:ARKSessionRunning];
}

- (void)runSessionWithAppState:(AppState *)state {
    [self updateARConfigurationWithState:state];
    [[self session] runWithConfiguration:[self configuration]];
    [self setArSessionState:ARKSessionRunning];
}

- (void)setShowMode:(ShowMode)showMode
{
    _showMode = showMode;
    
    [[self controller] setShowMode:showMode];
}

- (void)setShowOptions:(ShowOptions)showOptions
{
    _showOptions = showOptions;
    
    [[self controller] setShowOptions:showOptions];
}

- (NSArray *)hitTestNormPoint:(CGPoint)normPoint types:(NSUInteger)type
{
    CGSize renderSize = [[[self controller] renderView] bounds].size;
    
    CGPoint point = CGPointMake(normPoint.x * renderSize.width, normPoint.y * renderSize.height);
    
    NSArray *result = [[self controller] hitTest:point withType:type];
    
    return hitTestResultArrayFromResult(result);
}

- (BOOL)addAnchor:(NSString *)userGeneratedAnchorID transform:(NSArray *)transform
{
    if ((userGeneratedAnchorID == nil) || [[arkitGeneratedAnchorIDUserAnchorIDMap allValues] containsObject: userGeneratedAnchorID])
    {
        DDLogError(@"Duplicate or NIL anchor Name - %@", userGeneratedAnchorID);
        return NO;
    }
    
    matrix_float4x4 matrix = [transform isKindOfClass:[NSArray class]] ? matrixFromArray(transform) : matrixFromDictionary((NSDictionary *)transform);
    
    ARAnchor *anchor = [[ARAnchor alloc] initWithName:userGeneratedAnchorID transform:matrix];
    
    [[self session] addAnchor:anchor];

    arkitGeneratedAnchorIDUserAnchorIDMap[[[anchor identifier] UUIDString]] = userGeneratedAnchorID;

    return YES;
}

- (void)removeAnchors:(NSArray *)anchorIDsToDelete {
    for (NSString *anchorIDToDelete in anchorIDsToDelete) {
        ARAnchor *anchorToDelete = [self getAnchorFromUserAnchorID:anchorIDToDelete];
        if (anchorToDelete) {
            [self.session removeAnchor:anchorToDelete];
        } else {
            anchorToDelete = [self getAnchorFromARKitAnchorID:anchorIDToDelete];
            if (anchorToDelete) {
                [self.session removeAnchor:anchorToDelete];
            }
        }
    }
}

- (ARAnchor *)getAnchorFromARKitAnchorID:(NSString *)arkitAnchorID {
    ARAnchor *anchor = nil;
    ARFrame *currentFrame = [[self session] currentFrame];
    for (ARAnchor *currentAnchor in [currentFrame anchors]) {
        if ([[currentAnchor.identifier UUIDString] isEqualToString:arkitAnchorID]) {
            anchor = currentAnchor;
            break;
        }
    }
    return anchor;
}

- (ARAnchor *)getAnchorFromUserAnchorID:(NSString *)userAnchorID {
    __block ARAnchor *anchor = nil;
    [arkitGeneratedAnchorIDUserAnchorIDMap enumerateKeysAndObjectsUsingBlock:^(NSString* arkitID, NSString* userID, BOOL *stop) {
        if ([userID isEqualToString:userAnchorID]) {
            ARFrame *currentFrame = [[self session] currentFrame];
            for (ARAnchor *currentAnchor in [currentFrame anchors]) {
                if ([[currentAnchor.identifier UUIDString] isEqualToString:arkitID]) {
                    anchor = currentAnchor;
                    break;
                }
            }
            *stop = YES;
        }
    }];
    return anchor;
}

- (void)removeDetectionImages {
    self.detectionImageActivationPromises = [NSMutableDictionary new];
    self.referenceImageMap = [NSMutableDictionary new];
    self.detectionImageCreationRequests = [NSMutableArray new];
    self.detectionImageCreationPromises = [NSMutableDictionary new];
    self.detectionImageActivationAfterRemovalPromises = [NSMutableDictionary new];
}

- (void)removeDistantAnchors {
    matrix_float4x4 cameraTransform = [[[self.session currentFrame] camera] transform];
    float distanceThreshold = [[NSUserDefaults standardUserDefaults] floatForKey:distantAnchorsDistanceKey];
    
    for (ARAnchor *anchor in [[self.session currentFrame] anchors]) {
        if ([anchor isKindOfClass:[ARPlaneAnchor class]]) {
            ARPlaneAnchor* planeAnchor = (ARPlaneAnchor*)anchor;
            matrix_float4x4 cameraMatrixInAnchorCoordinates = matrix_multiply(matrix_invert(anchor.transform), cameraTransform);
            simd_float4 cameraPositionInAnchorCoordinates = cameraMatrixInAnchorCoordinates.columns[3];
            simd_float4 cameraPositionRelativeToPlaneCenter = cameraPositionInAnchorCoordinates - simd_make_float4(planeAnchor.center, 1.0);
            
            NSLog(@"cam plane coords:\t %f, %f, %f", cameraPositionRelativeToPlaneCenter[0], cameraPositionRelativeToPlaneCenter[1], cameraPositionRelativeToPlaneCenter[2]);
            NSLog(@"extents:\t\t\t %f, %f, %f", planeAnchor.extent[0], planeAnchor.extent[1], planeAnchor.extent[2]);
            NSLog(@"center:\t\t\t\t %f, %f, %f", planeAnchor.center[0], planeAnchor.center[1], planeAnchor.center[2]);
            if ((cameraPositionRelativeToPlaneCenter[0] - planeAnchor.extent[0] > distanceThreshold) ||
                (cameraPositionRelativeToPlaneCenter[0] + planeAnchor.extent[0] < -distanceThreshold) ||
                
                (cameraPositionRelativeToPlaneCenter[1] - planeAnchor.extent[1] > distanceThreshold) ||
                (cameraPositionRelativeToPlaneCenter[1] + planeAnchor.extent[1] < -distanceThreshold) ||
                
                (cameraPositionRelativeToPlaneCenter[2] - planeAnchor.extent[2] > distanceThreshold) ||
                (cameraPositionRelativeToPlaneCenter[2] + planeAnchor.extent[2] < -distanceThreshold)
                ) {
                
                NSLog(@"\n\n*********\n\nRemoving distant plane %@\n\n*********", [anchor.identifier UUIDString]);
                [self.session removeAnchor:anchor];
            }
        } else {
            float distance = simd_distance(anchor.transform.columns[3], cameraTransform.columns[3]);
            if (distance >= distanceThreshold) {
                NSLog(@"\n\n*********\n\nRemoving distant anchor %@\n\n*********", [anchor.identifier UUIDString]);
                [self.session removeAnchor:anchor];
            }
        }
    }
}

- (void)removeAllAnchors {
    [self clearImageDetectionDictionaries];

    ARFrame *currentFrame = [[self session] currentFrame];

    for (ARAnchor *anchor in [currentFrame anchors]) {
        [[self session] removeAnchor:anchor];
    }
}

- (void)clearImageDetectionDictionaries {
    [self.detectionImageActivationPromises removeAllObjects];
    [self.referenceImageMap removeAllObjects];
    [self.detectionImageCreationRequests removeAllObjects];
    [self.detectionImageCreationPromises removeAllObjects];
    [self.detectionImageActivationAfterRemovalPromises removeAllObjects];
}

- (void)removeAllAnchorsExceptPlanes {
    [self clearImageDetectionDictionaries];

    ARFrame *currentFrame = [[self session] currentFrame];

    for (ARAnchor *anchor in [currentFrame anchors]) {
        if (![anchor isKindOfClass:[ARPlaneAnchor class]]) {
            [[self session] removeAnchor:anchor];
        }
    }
}

- (void)setSendingWorldSensingDataAuthorizationStatus:(SendWorldSensingDataAuthorizationState)authorizationStatus {
    _sendingWorldSensingDataAuthorizationStatus = authorizationStatus;
    
    switch (self.sendingWorldSensingDataAuthorizationStatus) {
        case SendWorldSensingDataAuthorizationStateNotDetermined: {
            NSLog(@"World sensing auth changed to not determined");
            break;
        }
        case SendWorldSensingDataAuthorizationStateAuthorized: {
            NSLog(@"World sensing auth changed to authorized");
            
            // make sure all the anchors are in the objects[] array, and mark them as added
            NSArray *anchors = [[[self session] currentFrame] anchors];
            for (ARAnchor* addedAnchor in anchors) {
                if (!objects[[self anchorIDForAnchor:addedAnchor]]) {
                    NSMutableDictionary *addedAnchorDictionary = [[self createDictionaryForAnchor:addedAnchor] mutableCopy];
                    [addedAnchorsSinceLastFrame addObject: addedAnchorDictionary];
                    objects[[self anchorIDForAnchor:addedAnchor]] = addedAnchorDictionary;
                }
            }
            
            [self createRequestedDetectionImages];

            // Only need to do this if there's an outstanding world map request
            if (self.getWorldMapPromise) {
                [self _getWorldMap];
            }
            break;
        }
        case SendWorldSensingDataAuthorizationStateDenied: {
            NSLog(@"World sensing auth changed to denied");

            // still need to send the "required" anchors
            NSArray *anchors = [[[self session] currentFrame] anchors];
            for (ARAnchor* addedAnchor in anchors) {
                if (objects[[self anchorIDForAnchor:addedAnchor]]) {
                    // if the anchor is in the current object list, and is now not being sent
                    // mark it as removed and remove from the object list
                    if (![self shouldSendAnchor: addedAnchor]) {
                        [removedAnchorsSinceLastFrame addObject:[self anchorIDForAnchor:addedAnchor]];
                        objects[[self anchorIDForAnchor:addedAnchor]] = nil;
                    }
                } else {
                    // if the anchor was not being sent but is in the approved list, start sending it
                    if ([self shouldSendAnchor: addedAnchor]) {
                        NSMutableDictionary *addedAnchorDictionary = [[self createDictionaryForAnchor:addedAnchor] mutableCopy];
                        [addedAnchorsSinceLastFrame addObject: addedAnchorDictionary];
                        objects[[self anchorIDForAnchor:addedAnchor]] = addedAnchorDictionary;
                    }
                }
            }
            
            if (self.getWorldMapPromise) {
                self.getWorldMapPromise(NO, @"The user denied access to world sensing data", nil);
                self.getWorldMapPromise = nil;
            }

            for (NSDictionary* referenceImageDictionary in self.detectionImageCreationRequests) {
                DetectionImageCreatedCompletionType block = self.detectionImageCreationPromises[referenceImageDictionary[@"uid"]];
                block(NO, @"The user denied access to world sensing data");
            }
            [self.detectionImageCreationRequests removeAllObjects];
            [self.detectionImageCreationPromises removeAllObjects];
            break;
        }
    }
}

- (void)createRequestedDetectionImages {
    for (NSDictionary* referenceImageDictionary in self.detectionImageCreationRequests) {
        [self _createDetectionImage:referenceImageDictionary];
    }
}

- (void)createDetectionImage:(NSDictionary *)referenceImageDictionary completion:(DetectionImageCreatedCompletionType)completion {
    switch (self.sendingWorldSensingDataAuthorizationStatus) {
        case SendWorldSensingDataAuthorizationStateAuthorized: {
            self.detectionImageCreationPromises[referenceImageDictionary[@"uid"]] = completion;
            [self _createDetectionImage:referenceImageDictionary];
            break;
        }
        case SendWorldSensingDataAuthorizationStateDenied: {
            completion(NO, @"The user denied access to world sensing data");
            break;
        }

        case SendWorldSensingDataAuthorizationStateNotDetermined: {
            NSLog(@"Attempt to create a detection image but world sensing data authorization is not determined, enqueue the request");
            self.detectionImageCreationPromises[referenceImageDictionary[@"uid"]] = completion;
            [self.detectionImageCreationRequests addObject: referenceImageDictionary];
            break;
        }
    }
}


- (void)_createDetectionImage:(NSDictionary *)referenceImageDictionary {
    ARReferenceImage *referenceImage = [self createReferenceImageFromDictionary:referenceImageDictionary];
    DetectionImageCreatedCompletionType block = self.detectionImageCreationPromises[referenceImageDictionary[@"uid"]];
    if (referenceImage) {
        self.referenceImageMap[referenceImage.name] = referenceImage;
        NSLog(@"Detection image created: %@", referenceImage.name);
        
        if (block) {
            block(YES, nil);
        }
    } else {
        NSLog(@"Cannot create detection image from dictionary: %@", referenceImageDictionary[@"uid"]);
        if (block) {
            block(NO, @"Error creating the ARReferenceImage");
        }
    }
    
    self.detectionImageCreationPromises[referenceImageDictionary[@"uid"]] = nil;
}

- (void)activateDetectionImage:(NSString *)imageName completion:(ActivateDetectionImageCompletionBlock)completion {
    if ([[self configuration] isKindOfClass:[ARFaceTrackingConfiguration class]]) {
        completion(NO, @"Cannot activate a detection image when using the front facing camera", nil);
        return;
    }
    
    ARWorldTrackingConfiguration* worldTrackingConfiguration = (ARWorldTrackingConfiguration*)[self configuration];
    ARReferenceImage *referenceImage = self.referenceImageMap[imageName];
    if (referenceImage) {
        NSMutableSet* currentDetectionImages = [worldTrackingConfiguration detectionImages] != nil ? [[worldTrackingConfiguration detectionImages] mutableCopy] : [NSMutableSet new];
        if (![currentDetectionImages containsObject:referenceImage]) {
            [currentDetectionImages addObject: referenceImage];
            [worldTrackingConfiguration setDetectionImages: currentDetectionImages];
            
            self.detectionImageActivationPromises[referenceImage.name] = completion;
            [[self session] runWithConfiguration:[self configuration]];
        } else {
            if (self.detectionImageActivationPromises[referenceImage.name]) {
                // Trying to activate an image that hasn't been activated yet, return an error on the second promise, but keep the first
                completion(NO, @"Trying to activate an image that's already activated but not found yet", nil);
                return;
            } else {
                // Activating an already activated and found image, remove the anchor from the scene
                // so it can be detected again
                for(ARAnchor* anchor in self.session.currentFrame.anchors) {
                    if ([anchor isKindOfClass:[ARImageAnchor class]]) {
                        ARImageAnchor* imageAnchor = (ARImageAnchor*)anchor;
                        if ([imageAnchor.referenceImage.name isEqualToString:imageName]) {
                            // Remove the reference image fromt he session configuration and run again
                            [currentDetectionImages removeObject:referenceImage];
                            [worldTrackingConfiguration setDetectionImages: currentDetectionImages];
                            [[self session] runWithConfiguration:[self configuration]];
                            
                            // When the anchor is removed and didRemoveAnchor callback gets called, look in this map
                            // and see if there is a promise for the recently removed image anchor. If so, call
                            // activateDetectionImage again with the image name of the removed anchor, and the completion set here
                            self.detectionImageActivationAfterRemovalPromises[referenceImage.name] = completion;
                            [self.session removeAnchor: anchor];
                            return;
                        }
                    }
                    
                }
            }
        }
    } else {
        completion(NO, [NSString stringWithFormat:@"The image %@ doesn't exist", imageName], nil);
    }
}

- (void)deactivateDetectionImage:(NSString *)imageName completion:(DetectionImageCreatedCompletionType)completion {
    if ([[self configuration] isKindOfClass:[ARFaceTrackingConfiguration class]]) {
        completion(NO, @"Cannot deactivate a detection image when using the front facing camera");
        return;
    }
    
    ARWorldTrackingConfiguration* worldTrackingConfiguration = (ARWorldTrackingConfiguration*)[self configuration];
    ARReferenceImage *referenceImage = self.referenceImageMap[imageName];

    NSMutableSet* currentDetectionImages = [worldTrackingConfiguration detectionImages] != nil ? [[worldTrackingConfiguration detectionImages] mutableCopy] : [NSMutableSet new];
    if ([currentDetectionImages containsObject:referenceImage]) {
        if (self.detectionImageActivationPromises[referenceImage.name]) {
            NSLog(@"The image trying to deactivate is activated and hasn't been found yet, return error");
            // The image trying to deactivate hasn't been found yet, return an error on the activation block and remove it
            ActivateDetectionImageCompletionBlock activationBlock = self.detectionImageActivationPromises[referenceImage.name];
            activationBlock(NO, @"The image has been deactivated", nil);
            self.detectionImageActivationPromises[referenceImage.name] = nil;
            return;
        }
        
        [currentDetectionImages removeObject: referenceImage];
        [worldTrackingConfiguration setDetectionImages: currentDetectionImages];

        self.detectionImageActivationPromises[referenceImage.name] = nil;
        [[self session] runWithConfiguration:[self configuration]];
        completion(YES, nil);
    } else {
        completion(NO, @"The image trying to deactivate doesn't exist");
    }
}

- (void)destroyDetectionImage:(NSString *)imageName completion:(DetectionImageCreatedCompletionType)completion {
    ARReferenceImage *referenceImage = self.referenceImageMap[imageName];
    if (referenceImage) {
        self.referenceImageMap[imageName] = nil;
        self.detectionImageActivationPromises[imageName] = nil;

        completion(YES, nil);
    } else {
        completion(NO, @"The image doesn't exist");
    }
}

- (ARReferenceImage*)createReferenceImageFromDictionary:(NSDictionary*)referenceImageDictionary {
    CGFloat physicalWidth = [referenceImageDictionary[@"physicalWidth"] doubleValue];
    NSString* b64String = referenceImageDictionary[@"buffer"];
    size_t width = (size_t) [referenceImageDictionary[@"imageWidth"] intValue];
    size_t height = (size_t) [referenceImageDictionary[@"imageHeight"] intValue];
    size_t bitsPerComponent = 8;
    size_t bitsPerPixel = 32;
    size_t bytesPerRow = width * 4;
    CGColorSpaceRef colorSpace = CGColorSpaceCreateWithName(kCGColorSpaceSRGB);
    CGBitmapInfo bitmapInfo = (CGBitmapInfo) 0;
    NSData *data = [[NSData alloc] initWithBase64EncodedString:b64String options:NSDataBase64DecodingIgnoreUnknownCharacters];
    CFDataRef bridgedData  = (__bridge CFDataRef)data;
    CGDataProviderRef dataProvider = CGDataProviderCreateWithCFData(bridgedData);
    
    BOOL shouldInterpolate = YES;
    
    
    CGImageRef cgImage = CGImageCreate(width, height, bitsPerComponent, bitsPerPixel, bytesPerRow,
                                       colorSpace, bitmapInfo,
                                       dataProvider, NULL, shouldInterpolate,
                                       kCGRenderingIntentDefault);
    ARReferenceImage* result = [[ARReferenceImage alloc] initWithCGImage:cgImage orientation:kCGImagePropertyOrientationUp physicalWidth:physicalWidth];
    result.name = referenceImageDictionary[@"uid"];
    
    CGDataProviderRelease(dataProvider);
    CGColorSpaceRelease(colorSpace);
    return result;
}

- (void)switchCameraButtonTapped {
    for (ARAnchor* anchor in self.session.currentFrame.anchors) {
        [self.session removeAnchor: anchor];
    }
    
    if (![[self configuration] isKindOfClass:[ARFaceTrackingConfiguration class]]) {
        ARFaceTrackingConfiguration* faceTrackingConfiguration = [ARFaceTrackingConfiguration new];
        self.configuration = faceTrackingConfiguration;
        [self.session runWithConfiguration: self.configuration];
    } else {
        ARWorldTrackingConfiguration* worldTrackingConfiguration = [ARWorldTrackingConfiguration new];
        [worldTrackingConfiguration setPlaneDetection:ARPlaneDetectionHorizontal | ARPlaneDetectionVertical];
        [worldTrackingConfiguration setWorldAlignment:ARWorldAlignmentGravityAndHeading];

        // Configure all the active images that weren't detected in the previous back camera session
        NSArray *notDetectedImageNames = [self.detectionImageActivationPromises allKeys];
        NSMutableSet* newDetectionImages = [NSMutableSet new];
        for (NSString* imageName in notDetectedImageNames) {
            ARReferenceImage *referenceImage = self.referenceImageMap[imageName];
            if (referenceImage) {
                [newDetectionImages addObject:referenceImage];
            }
        }
        worldTrackingConfiguration.detectionImages = [newDetectionImages copy];

        self.configuration = worldTrackingConfiguration;
        [self.session runWithConfiguration: self.configuration];
    }
}

+ (BOOL)supportsARFaceTrackingConfiguration {
    return [ARFaceTrackingConfiguration isSupported];
}

#pragma mark Private

- (void)updateARKDataWithFrame:(ARFrame *)frame
{
    @synchronized(self)
    {
        if ([self request] == nil)
        {
            return;
        }
        
        if (frame)
        {
            NSMutableDictionary *newData = [NSMutableDictionary dictionaryWithCapacity:3]; // max request object
            NSInteger ts = (NSInteger) ([frame timestamp] * 1000.0);
            newData[@"timestamp"] = @(ts);

            // status of ARKit World Mapping
            newData[WEB_AR_WORLDMAPPING_STATUS_MESSAGE] = worldMappingState(frame);
            
            if ([[self request][WEB_AR_LIGHT_INTENSITY_OPTION] boolValue])
            {
                newData[WEB_AR_LIGHT_INTENSITY_OPTION] = @([[frame lightEstimate] ambientIntensity]);
                
                NSMutableDictionary* lightDictionary = [NSMutableDictionary new];
                lightDictionary[WEB_AR_LIGHT_INTENSITY_OPTION] = @([[frame lightEstimate] ambientIntensity]);
                lightDictionary[WEB_AR_LIGHT_AMBIENT_COLOR_TEMPERATURE_OPTION] = @([[frame lightEstimate] ambientColorTemperature]);
                
                if ([[frame lightEstimate] isKindOfClass:[ARDirectionalLightEstimate class]]) {
                    ARDirectionalLightEstimate* directionalLightEstimate = (ARDirectionalLightEstimate*)[frame lightEstimate];
                    lightDictionary[WEB_AR_PRIMARY_LIGHT_DIRECTION_OPTION] = @{
                                                                               @"x": @(directionalLightEstimate.primaryLightDirection[0]),
                                                                               @"y": @(directionalLightEstimate.primaryLightDirection[1]),
                                                                               @"z": @(directionalLightEstimate.primaryLightDirection[2])
                                                                               };
                    lightDictionary[WEB_AR_PRIMARY_LIGHT_INTENSITY_OPTION] = @(directionalLightEstimate.primaryLightIntensity);
                    
                }
                newData[WEB_AR_LIGHT_OBJECT_OPTION] = lightDictionary;
            }
            if ([[self request][WEB_AR_CAMERA_OPTION] boolValue])
            {
                CGSize size = [[self controller] renderView].frame.size;
                matrix_float4x4 projectionMatrix = [[frame camera] projectionMatrixForOrientation:self.interfaceOrientation
                                                                               viewportSize:size
                                                                                      zNear:AR_CAMERA_PROJECTION_MATRIX_Z_NEAR
                                                                                       zFar:AR_CAMERA_PROJECTION_MATRIX_Z_FAR];
                newData[WEB_AR_PROJ_CAMERA_OPTION] = arrayFromMatrix4x4(projectionMatrix);
             
                matrix_float4x4 viewMatrix = [frame.camera viewMatrixForOrientation:self.interfaceOrientation];
                matrix_float4x4 modelMatrix = matrix_invert(viewMatrix);
                
                newData[WEB_AR_CAMERA_TRANSFORM_OPTION] = arrayFromMatrix4x4(modelMatrix);
                newData[WEB_AR_CAMERA_VIEW_OPTION] = arrayFromMatrix4x4(viewMatrix);
            }
            if ([[self request][WEB_AR_3D_OBJECTS_OPTION] boolValue])
            {
                NSArray* anchorsArray = [self currentAnchorsArray];
                newData[WEB_AR_3D_OBJECTS_OPTION] = anchorsArray;

                // Prepare the objectsRemoved array
                NSArray *removedObjects = [removedAnchorsSinceLastFrame copy];
                [removedAnchorsSinceLastFrame removeAllObjects];
                newData[WEB_AR_3D_REMOVED_OBJECTS_OPTION] = removedObjects;
                
                // Prepare the newObjects array
                NSArray *newObjects = [addedAnchorsSinceLastFrame copy];
                [addedAnchorsSinceLastFrame removeAllObjects];
                newData[WEB_AR_3D_NEW_OBJECTS_OPTION] = newObjects;
            }
            
            NSInteger frameTimestamp = (NSInteger) ([frame timestamp] * 1000.0);

            if ([self computerVisionDataEnabled] && frameTimestamp - lastConvertTime > (1000/CVIMAGE_FPS)) {
                        lastConvertTime = frameTimestamp;
                NSMutableDictionary *cameraInformation = [NSMutableDictionary new];
                CGSize cameraImageResolution = [[frame camera] imageResolution];
                cameraInformation[@"cameraImageResolution"] = @{
                                                                @"width": @(cameraImageResolution.width),
                                                                @"height": @(cameraImageResolution.height)
                                                                };
                
                
                matrix_float3x3 cameraIntrinsics = [[frame camera] intrinsics];
                matrix_float3x3 resizedCameraIntrinsics = [[frame camera] intrinsics];
                for (int i = 0; i < 3; i++) {
                    for (int j = 0; j < 3; j++) {
                        resizedCameraIntrinsics.columns[i][j] = cameraIntrinsics.columns[i][j]/self.computerVisionImageScaleFactor;
                    }
                }
                resizedCameraIntrinsics.columns[2][2] = 1.0f;

                cameraInformation[@"cameraIntrinsics"] = arrayFromMatrix3x3(resizedCameraIntrinsics);
                
                // Get the projection matrix
                CGSize viewportSize = [[self controller] renderView].frame.size;
                matrix_float4x4 projectionMatrix = [[frame camera] projectionMatrixForOrientation:self.interfaceOrientation
                                                                                     viewportSize:viewportSize
                                                                                            zNear:AR_CAMERA_PROJECTION_MATRIX_Z_NEAR
                                                                                             zFar:AR_CAMERA_PROJECTION_MATRIX_Z_FAR];
                cameraInformation[@"projectionMatrix"] = arrayFromMatrix4x4(projectionMatrix);
                
                // Get the view matrix
                matrix_float4x4 viewMatrix = [frame.camera viewMatrixForOrientation:self.interfaceOrientation];
                cameraInformation[@"viewMatrix"] = arrayFromMatrix4x4(viewMatrix);
                
                cameraInformation[@"inverse_viewMatrix"] = arrayFromMatrix4x4(matrix_invert(viewMatrix));
                
                
                // Send also the interface orientation
                cameraInformation[@"interfaceOrientation"] = @(self.interfaceOrientation);
                
                NSMutableDictionary *cvInformation = [NSMutableDictionary new];
                NSMutableDictionary *frameInformation = [NSMutableDictionary new];
                frameInformation[@"timestamp"] = @(frameTimestamp);
                
                // TODO: prepare depth data
                frameInformation[@"capturedDepthData"] = nil;
                frameInformation[@"capturedDepthDataTimestamp"] = nil;
                
                // Computer vision data
//                [self updateBase64BuffersFromPixelBuffer:frame.capturedImage];
            
                NSMutableDictionary *lumaBufferDictionary = [NSMutableDictionary new];
                lumaBufferDictionary[@"size"] = @{
                                        @"width": @(self.lumaBufferSize.width),
                                        @"height": @(self.lumaBufferSize.height),
                                        @"bytesPerRow": @(self.lumaBufferSize.width * sizeof(Pixel_8)),
                                        @"bytesPerPixel": @(sizeof(Pixel_8))
                                        };
//                lumaBufferDictionary[@"buffer"] = self.lumaBase64StringBuffer;


                NSMutableDictionary *chromaBufferDictionary = [NSMutableDictionary new];
                chromaBufferDictionary[@"size"] = @{
                                        @"width": @(self.chromaBufferSize.width),
                                        @"height": @(self.chromaBufferSize.height),
                                        @"bytesPerRow": @(self.chromaBufferSize.width * sizeof(Pixel_16U)),
                                        @"bytesPerPixel": @(sizeof(Pixel_16U))
                                        };
//                chromaBufferDictionary[@"buffer"] = self.chromaBase64StringBuffer;
                
//                NSMutableDictionary *jpegBufferDictionary = [NSMutableDictionary new];
//                jpegBufferDictionary[@"data"] = [self getBase64ImageFromPixelBuffer:frame.capturedImage];
                
//                frameInformation[@"buffers"] = @[lumaBufferDictionary, chromaBufferDictionary, jpegBufferDictionary];
//                frameInformation[@"pixelFormatType"] = [self stringForOSType:CVPixelBufferGetPixelFormatType(frame.capturedImage)];
//                frameInformation[@"clinkTag"]
                
                cvInformation[@"frame"] = frameInformation;
                cvInformation[@"camera"] = cameraInformation;
                cvInformation[@"clinkCode"] = [[self controller] clinkCode];
                NSLog(@"%@",[[self controller] clinkCode]);
                
//                os_unfair_lock_lock(&(lock));
//                computerVisionData = [cvInformation copy];
//                os_unfair_lock_unlock(&(lock));
                computerVisionData = cvInformation;
            }

            newData[WEB_AR_3D_GEOALIGNED_OPTION] = @([[self configuration] worldAlignment] == ARWorldAlignmentGravityAndHeading ? YES : NO);
            newData[WEB_AR_3D_VIDEO_ACCESS_OPTION] = @([self computerVisionDataEnabled] ? YES : NO);
            
//            os_unfair_lock_lock(&(lock));
//            arkData = [newData copy];
//            os_unfair_lock_unlock(&(lock));
            arkData = newData;
        }
    }
}

-(void)logPixelBufferInfo:(CVPixelBufferRef)capturedImagePixelBuffer {
    size_t capturedImagePixelBufferWidth = CVPixelBufferGetWidth(capturedImagePixelBuffer);
    size_t capturedImagePixelBufferHeight = CVPixelBufferGetHeight(capturedImagePixelBuffer);
    size_t capturedImagePixelBufferBytesPerRow = CVPixelBufferGetBytesPerRow(capturedImagePixelBuffer);
    size_t capturedImageNumberOfPlanes = CVPixelBufferGetPlaneCount(capturedImagePixelBuffer);
    CFTypeID capturedImagePixelBufferTypeID = CVPixelBufferGetTypeID();
    size_t capturedImagePixelBufferDataSize = CVPixelBufferGetDataSize(capturedImagePixelBuffer);
    OSType capturedImagePixelBufferPixelFormatType = CVPixelBufferGetPixelFormatType(capturedImagePixelBuffer);
    void* capturedImagePixelBufferBaseAddress = CVPixelBufferGetBaseAddress(capturedImagePixelBuffer);

    NSLog(@"\n\nnumberOfPlanes: %zu\npixelBufferWidth: %zu\npixelBufferHeight: %zu\npixelBufferTypeID: %lu\npixelBufferDataSize: %zu\npixelBufferBytesPerRow: %zu\npixelBufferPIxelFormatType: %@\npixelBufferBaseAddress: %p\n",
          capturedImageNumberOfPlanes,
          capturedImagePixelBufferWidth,
          capturedImagePixelBufferHeight,
          capturedImagePixelBufferTypeID,
          capturedImagePixelBufferDataSize,
          capturedImagePixelBufferBytesPerRow,
          [self stringForOSType:capturedImagePixelBufferPixelFormatType],
          capturedImagePixelBufferBaseAddress);
}

/*
 This code is inspired by the open source project:
 https://github.com/Stinkstudios/arkit-web
 Kudos to: Amelie (@ixviii_io)
 */
-(NSString*)getBase64ImageFromPixelBuffer:(CVPixelBufferRef)pixelBuffer {
    // The context to be able to create the CGImage.
    static CIContext* ciContext = nil;
    ciContext = [CIContext contextWithOptions:nil];
    // Convert the pixel buffer to a CIImage
    CIImage* ciImage = [CIImage imageWithCVPixelBuffer:pixelBuffer];
    // Apply a scaling transformation to the CIImage and get a new one
    CGAffineTransform scaleTransform =
    CGAffineTransformScale(CGAffineTransformIdentity,
                           CAMERA_FRAME_SCALE_FACTOR, CAMERA_FRAME_SCALE_FACTOR);
    CIImage* resizedCIImage = [ciImage imageByApplyingTransform:scaleTransform];
    // Create a CGImage from the CIImage
    CGImageRef cgImage = [ciContext createCGImage:resizedCIImage
                                         fromRect:resizedCIImage.extent];
    if (cgImage) {
        // Create an UIImage from the CGImage
        UIImage* uiImage = [UIImage imageWithCGImage:cgImage];
        // IMPORTANT: CG structures are not handled by the ARC system.
        // Release the CG image now that we have a corresponding UIImage.
        CGImageRelease(cgImage);
        // Compress the image as JPEG
        NSData* jpegImageData =
        UIImageJPEGRepresentation(uiImage, CAMERA_FRAME_JPEG_COMPRESSION_FACTOR);
        if (jpegImageData) {
            // Transform the JPEG data into a base64 format so it can be
            // passed to the JS side as a string.
            return [jpegImageData base64EncodedStringWithOptions:
                    NSDataBase64Encoding64CharacterLineLength];
        }
    }
    return nil;
}

-(void)updateBase64BuffersFromPixelBuffer:(CVPixelBufferRef)capturedImagePixelBuffer {

    // Luma
    CVPixelBufferLockBaseAddress(capturedImagePixelBuffer, kCVPixelBufferLock_ReadOnly);
    
    //[self logPixelBufferInfo:capturedImagePixelBuffer];

    size_t lumaBufferWidth = CVPixelBufferGetWidthOfPlane(capturedImagePixelBuffer, 0);
    size_t lumaBufferHeight = CVPixelBufferGetHeightOfPlane(capturedImagePixelBuffer, 0);
    
    vImage_Buffer lumaSrcBuffer;
    lumaSrcBuffer.data = CVPixelBufferGetBaseAddressOfPlane(capturedImagePixelBuffer, 0);
    lumaSrcBuffer.width = lumaBufferWidth;
    lumaSrcBuffer.height = lumaBufferHeight;
    lumaSrcBuffer.rowBytes = CVPixelBufferGetBytesPerRowOfPlane(capturedImagePixelBuffer, 0);
    
    size_t extraColumnsOnLeft;
    size_t extraColumnsOnRight;
    size_t extraColumnsOnTop;
    size_t extraColumnsOnBottom;
    CVPixelBufferGetExtendedPixels(capturedImagePixelBuffer, &extraColumnsOnLeft, &extraColumnsOnRight, &extraColumnsOnTop, &extraColumnsOnBottom);
    
    if (self.lumaBufferSize.width == 0.0f) {
        self.lumaBufferSize = [self downscaleByFactorOf2UntilLargestSideIsLessThan512AvoidingFractionalSides: CGSizeMake(lumaBufferWidth, lumaBufferHeight)];
    }
    self.chromaBufferSize = CGSizeMake(self.lumaBufferSize.width/2.0, self.lumaBufferSize.height/2.0);
    
    if (self.lumaBuffer.data == nil) {
        vImageBuffer_Init(&self->_lumaBuffer, self.lumaBufferSize.height, self.lumaBufferSize.width, 8 * sizeof(Pixel_8), kvImageNoFlags);
        vImageScale_Planar8(&self->_lumaBuffer, &self->_lumaBuffer, NULL, kvImageGetTempBufferSize);
        size_t scaledBufferSize = vImageScale_Planar8(&lumaSrcBuffer, &self->_lumaBuffer, NULL, kvImageGetTempBufferSize);
        self.lumaScaleTemporaryBuffer = malloc(scaledBufferSize * sizeof(Pixel_8));
    }

    vImage_Error scaleError = vImageScale_Planar8(&lumaSrcBuffer, &self->_lumaBuffer, self.lumaScaleTemporaryBuffer, kvImageNoFlags);
    if (scaleError != 0) {
        NSLog(@"Error scaling luma image");
        CVPixelBufferUnlockBaseAddress(capturedImagePixelBuffer, kCVPixelBufferLock_ReadOnly);
        return;
    }
    
    if (self.lumaDataBuffer == nil) {
        self.lumaDataBuffer = [NSMutableData dataWithBytes:self.lumaBuffer.data
                                                    length:self.lumaBuffer.width * self.lumaBuffer.height * sizeof(Pixel_8)];
    }
    for (int currentRow = 0; currentRow < self.lumaBuffer.height; currentRow++) {
        [self.lumaDataBuffer replaceBytesInRange:NSMakeRange(self.lumaBuffer.width * currentRow, self.lumaBuffer.width)
                                       withBytes:self.lumaBuffer.data + self.lumaBuffer.rowBytes * currentRow];
    }
    
    if (self.lumaBase64StringBuffer == nil) {
        self.lumaBase64StringBuffer = [NSMutableString new];
    }
    [self.lumaBase64StringBuffer setString:[self.lumaDataBuffer base64EncodedStringWithOptions:0]];
    

    // Chroma
    vImage_Buffer chromaSrcBuffer;
    chromaSrcBuffer.data = CVPixelBufferGetBaseAddressOfPlane(capturedImagePixelBuffer, 1);
    chromaSrcBuffer.width = CVPixelBufferGetWidthOfPlane(capturedImagePixelBuffer, 1);
    chromaSrcBuffer.height = CVPixelBufferGetHeightOfPlane(capturedImagePixelBuffer, 1);
    chromaSrcBuffer.rowBytes = CVPixelBufferGetBytesPerRowOfPlane(capturedImagePixelBuffer, 1);
    
    if (self->_chromaBuffer.data == nil) {
        vImageBuffer_Init(&self->_chromaBuffer, self.chromaBufferSize.height, self.chromaBufferSize.width, 8 * sizeof(Pixel_16U), kvImageNoFlags);
        size_t scaledBufferSize = vImageScale_Planar8(&chromaSrcBuffer, &self->_chromaBuffer, NULL, kvImageGetTempBufferSize);
        self.chromaScaleTemporaryBuffer = malloc(scaledBufferSize * sizeof(Pixel_16U));
    }

    scaleError = vImageScale_CbCr8(&chromaSrcBuffer, &self->_chromaBuffer, self.chromaScaleTemporaryBuffer, kvImageNoFlags);
    if (scaleError != 0) {
        NSLog(@"Error scaling chroma image");
        CVPixelBufferUnlockBaseAddress(capturedImagePixelBuffer, kCVPixelBufferLock_ReadOnly);
        return;
    }

    if (self.chromaDataBuffer == nil) {
        self.chromaDataBuffer = [NSMutableData dataWithBytes:self.chromaBuffer.data
                                                      length:self.chromaBuffer.width * self.chromaBuffer.height * sizeof(Pixel_16U)];
    }
    for (int currentRow = 0; currentRow < self.chromaBuffer.height; currentRow++) {
        [self.chromaDataBuffer replaceBytesInRange:NSMakeRange(self.chromaBuffer.width * currentRow * sizeof(Pixel_16U), self.chromaBuffer.width * sizeof(Pixel_16U))
                                         withBytes:self.chromaBuffer.data + self.chromaBuffer.rowBytes * currentRow];
    }

    if (self.chromaBase64StringBuffer == nil) {
        self.chromaBase64StringBuffer = [NSMutableString new];
    }
    [self.chromaBase64StringBuffer setString:[self.chromaDataBuffer base64EncodedStringWithOptions:0]];
    
    CVPixelBufferUnlockBaseAddress(capturedImagePixelBuffer, kCVPixelBufferLock_ReadOnly);
}

- (CGSize)downscaleByFactorOf2UntilLargestSideIsLessThan512AvoidingFractionalSides:(CGSize)originalSize {
    CGSize result = originalSize;

    BOOL largestSideLessThan512Found = NO;
    BOOL fractionalSideFound = NO;
    self.computerVisionImageScaleFactor = 1.0;
    while (!(largestSideLessThan512Found || fractionalSideFound)) {
        if ((int)result.width%2 != 0 || (int)result.height%2 != 0) {
            fractionalSideFound = YES;
        } else {
            result = CGSizeMake(result.width/2.0, result.height/2.0);
            self.computerVisionImageScaleFactor *= 2.0;

            CGFloat largestSide = MAX(result.width, result.height);
            if (largestSide < 512) {
                largestSideLessThan512Found = YES;
            }
        }
    }

    return result;
}

- (NSString *)stringForOSType:(OSType)type {
    switch (type) {
        case kCVPixelFormatType_1Monochrome:                   return @"kCVPixelFormatType_1Monochrome";
        case kCVPixelFormatType_2Indexed:                      return @"kCVPixelFormatType_2Indexed";
        case kCVPixelFormatType_4Indexed:                      return @"kCVPixelFormatType_4Indexed";
        case kCVPixelFormatType_8Indexed:                      return @"kCVPixelFormatType_8Indexed";
        case kCVPixelFormatType_1IndexedGray_WhiteIsZero:      return @"kCVPixelFormatType_1IndexedGray_WhiteIsZero";
        case kCVPixelFormatType_2IndexedGray_WhiteIsZero:      return @"kCVPixelFormatType_2IndexedGray_WhiteIsZero";
        case kCVPixelFormatType_4IndexedGray_WhiteIsZero:      return @"kCVPixelFormatType_4IndexedGray_WhiteIsZero";
        case kCVPixelFormatType_8IndexedGray_WhiteIsZero:      return @"kCVPixelFormatType_8IndexedGray_WhiteIsZero";
        case kCVPixelFormatType_16BE555:                       return @"kCVPixelFormatType_16BE555";
        case kCVPixelFormatType_16LE555:                       return @"kCVPixelFormatType_16LE555";
        case kCVPixelFormatType_16LE5551:                      return @"kCVPixelFormatType_16LE5551";
        case kCVPixelFormatType_16BE565:                       return @"kCVPixelFormatType_16BE565";
        case kCVPixelFormatType_16LE565:                       return @"kCVPixelFormatType_16LE565";
        case kCVPixelFormatType_24RGB:                         return @"kCVPixelFormatType_24RGB";
        case kCVPixelFormatType_24BGR:                         return @"kCVPixelFormatType_24BGR";
        case kCVPixelFormatType_32ARGB:                        return @"kCVPixelFormatType_32ARGB";
        case kCVPixelFormatType_32BGRA:                        return @"kCVPixelFormatType_32BGRA";
        case kCVPixelFormatType_32ABGR:                        return @"kCVPixelFormatType_32ABGR";
        case kCVPixelFormatType_32RGBA:                        return @"kCVPixelFormatType_32RGBA";
        case kCVPixelFormatType_64ARGB:                        return @"kCVPixelFormatType_64ARGB";
        case kCVPixelFormatType_48RGB:                         return @"kCVPixelFormatType_48RGB";
        case kCVPixelFormatType_32AlphaGray:                   return @"kCVPixelFormatType_32AlphaGray";
        case kCVPixelFormatType_16Gray:                        return @"kCVPixelFormatType_16Gray";
        case kCVPixelFormatType_30RGB:                         return @"kCVPixelFormatType_30RGB";
        case kCVPixelFormatType_422YpCbCr8:                    return @"kCVPixelFormatType_422YpCbCr8";
        case kCVPixelFormatType_4444YpCbCrA8:                  return @"kCVPixelFormatType_4444YpCbCrA8";
        case kCVPixelFormatType_4444YpCbCrA8R:                 return @"kCVPixelFormatType_4444YpCbCrA8R";
        case kCVPixelFormatType_4444AYpCbCr8:                  return @"kCVPixelFormatType_4444AYpCbCr8";
        case kCVPixelFormatType_4444AYpCbCr16:                 return @"kCVPixelFormatType_4444AYpCbCr16";
        case kCVPixelFormatType_444YpCbCr8:                    return @"kCVPixelFormatType_444YpCbCr8";
        case kCVPixelFormatType_422YpCbCr16:                   return @"kCVPixelFormatType_422YpCbCr16";
        case kCVPixelFormatType_422YpCbCr10:                   return @"kCVPixelFormatType_422YpCbCr10";
        case kCVPixelFormatType_444YpCbCr10:                   return @"kCVPixelFormatType_444YpCbCr10";
        case kCVPixelFormatType_420YpCbCr8Planar:              return @"kCVPixelFormatType_420YpCbCr8Planar";
        case kCVPixelFormatType_420YpCbCr8PlanarFullRange:     return @"kCVPixelFormatType_420YpCbCr8PlanarFullRange";
        case kCVPixelFormatType_422YpCbCr_4A_8BiPlanar:        return @"kCVPixelFormatType_422YpCbCr_4A_8BiPlanar";
        case kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange:  return @"kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange";
        case kCVPixelFormatType_420YpCbCr8BiPlanarFullRange:   return @"kCVPixelFormatType_420YpCbCr8BiPlanarFullRange";
        case kCVPixelFormatType_422YpCbCr8_yuvs:               return @"kCVPixelFormatType_422YpCbCr8_yuvs";
        case kCVPixelFormatType_422YpCbCr8FullRange:           return @"kCVPixelFormatType_422YpCbCr8FullRange";
        case kCVPixelFormatType_OneComponent8:                 return @"kCVPixelFormatType_OneComponent8";
        case kCVPixelFormatType_TwoComponent8:                 return @"kCVPixelFormatType_TwoComponent8";
        case kCVPixelFormatType_30RGBLEPackedWideGamut:        return @"kCVPixelFormatType_30RGBLEPackedWideGamut";
        case kCVPixelFormatType_OneComponent16Half:            return @"kCVPixelFormatType_OneComponent16Half";
        case kCVPixelFormatType_OneComponent32Float:           return @"kCVPixelFormatType_OneComponent32Float";
        case kCVPixelFormatType_TwoComponent16Half:            return @"kCVPixelFormatType_TwoComponent16Half";
        case kCVPixelFormatType_TwoComponent32Float:           return @"kCVPixelFormatType_TwoComponent32Float";
        case kCVPixelFormatType_64RGBAHalf:                    return @"kCVPixelFormatType_64RGBAHalf";
        case kCVPixelFormatType_128RGBAFloat:                  return @"kCVPixelFormatType_128RGBAFloat";
        case kCVPixelFormatType_14Bayer_GRBG:                  return @"kCVPixelFormatType_14Bayer_GRBG";
        case kCVPixelFormatType_14Bayer_RGGB:                  return @"kCVPixelFormatType_14Bayer_RGGB";
        case kCVPixelFormatType_14Bayer_BGGR:                  return @"kCVPixelFormatType_14Bayer_BGGR";
        case kCVPixelFormatType_14Bayer_GBRG:                  return @"kCVPixelFormatType_14Bayer_GBRG";
        default: return @"UNKNOWN";
    }
}

- (NSArray *)currentAnchorsArray
{
    NSMutableArray *array = [NSMutableArray array];
    [objects enumerateKeysAndObjectsUsingBlock:^(id  _Nonnull key, id  _Nonnull obj, BOOL * _Nonnull stop)
     {
         if ([self sendingWorldSensingDataAuthorizationStatus] == SendWorldSensingDataAuthorizationStateAuthorized || [objects[key][WEB_AR_MUST_SEND_OPTION] boolValue]) {
             
             if ([objects[key][WEB_AR_ANCHOR_TYPE] isEqualToString:@"face"]) {
                 if (self.numberOfFramesWithoutSendingFaceGeometry < 1) {
                     self.numberOfFramesWithoutSendingFaceGeometry++;
                     NSMutableDictionary* mutableDict = [objects[key] mutableCopy];
                     [mutableDict removeObjectForKey:WEB_AR_GEOMETRY_OPTION];
                     objects[key] = mutableDict;
                 } else {
                     self.numberOfFramesWithoutSendingFaceGeometry = 0;
                 }
             }
             
             [array addObject:objects[key]];
         }
     }];
    
    return [array copy];
}

- (BOOL)hasPlanes
{
    ARFrame *currentFrame = [[self session] currentFrame];
    for (ARAnchor *anchor in [currentFrame anchors])
    {
        if ([anchor isKindOfClass:[ARPlaneAnchor class]])
        {
            return TRUE;
        }
    }
    return FALSE;
}

- (NSArray *)currentPlanesArray
{
    ARFrame *currentFrame = [[self session] currentFrame];
    
    NSMutableArray *array = [NSMutableArray array];
    
    for (ARAnchor *anchor in [currentFrame anchors])
    {
        if ([anchor isKindOfClass:[ARPlaneAnchor class]])
        {
            [array addObject:[self planeDictFromPlaneAnchor:(ARPlaneAnchor *)anchor]];
        }
    }
    
    return [array copy];
}

- (BOOL)trackingStateNormal {
    ARTrackingState ts = [[[[self session] currentFrame] camera] trackingState];
    return ts == ARTrackingStateNormal;
}

- (BOOL)worldMappingAvailable {
    ARWorldMappingStatus ws = [[[self session] currentFrame] worldMappingStatus];
    return ws != ARWorldMappingStatusNotAvailable;
}

- (NSString *)trackingState {
    return trackingState([[[self session] currentFrame] camera]);
}


- (NSDictionary *)planeDictFromPlaneAnchor:(ARPlaneAnchor *)planeAnchor
{
    NSMutableDictionary *dict = [NSMutableDictionary dictionaryWithCapacity:3];
    
    dict[WEB_AR_PLANE_ID_OPTION] = [[planeAnchor identifier] UUIDString];
    dict[WEB_AR_PLANE_CENTER_OPTION] = dictFromVector3([planeAnchor center]);
    dict[WEB_AR_PLANE_EXTENT_OPTION] = dictFromVector3([planeAnchor extent]);
    
    return [dict copy];
}

#pragma mark - ARSessionDelegate

- (void)session:(ARSession *)session didUpdateFrame:(ARFrame *)frame
{
    [self updateARKDataWithFrame:frame];
    
    [self didUpdate](self);

    if ([self shouldUpdateWindowSize]) {
        [self setShouldUpdateWindowSize:NO];
        if ([self didUpdateWindowSize]) {
            [self didUpdateWindowSize]();
        }
    }
}

- (void)session:(ARSession *)session didAddAnchors:(NSArray<ARAnchor*>*)anchors
{
    DDLogDebug(@"Add Anchors - %@", [anchors debugDescription]);
    for (ARAnchor* addedAnchor in anchors) {
        if ([addedAnchor isKindOfClass:[ARFaceAnchor class]] && ![self.configuration isKindOfClass:[ARFaceTrackingConfiguration class]]) {
            NSLog(@"Trying to add a face anchor to a session configuration that's not ARFaceTrackingConfiguration");
            continue;
        }
        
        if ([self shouldSendAnchor:addedAnchor] ||
            self.sendingWorldSensingDataAuthorizationStatus == SendWorldSensingDataAuthorizationStateAuthorized) {
            
            NSMutableDictionary *addedAnchorDictionary = [[self createDictionaryForAnchor:addedAnchor] mutableCopy];
            [addedAnchorsSinceLastFrame addObject: addedAnchorDictionary];
            objects[[self anchorIDForAnchor:addedAnchor]] = addedAnchorDictionary;
            
            if ([addedAnchor isKindOfClass:[ARImageAnchor class]]) {
                ARImageAnchor* addedImageAnchor = (ARImageAnchor*)addedAnchor;
                if ([[self.detectionImageActivationPromises allKeys] containsObject:addedImageAnchor.referenceImage.name]) {
                    // Call the detection image block
                    ActivateDetectionImageCompletionBlock block = self.detectionImageActivationPromises[addedImageAnchor.referenceImage.name];
                    block(YES, nil, addedAnchorDictionary);
                    self.detectionImageActivationPromises[addedImageAnchor.referenceImage.name] = nil;
                }
            }
        }
    }

    // Inform up in the calling hierarchy when we have plane anchors added to the scene
    if ([self didAddPlaneAnchors]) {
        if ([self anyPlaneAnchor:anchors]) {
            [self didAddPlaneAnchors]();
        }
    }
}

- (void) updateDictionaryForAnchor:(ARAnchor *)updatedAnchor {
    NSString* anchorID = [self anchorIDForAnchor:updatedAnchor];
    NSMutableDictionary* anchorDictionary = objects[anchorID];
    anchorDictionary[WEB_AR_TRANSFORM_OPTION] = arrayFromMatrix4x4([updatedAnchor transform]);

    if ([updatedAnchor isKindOfClass:[ARPlaneAnchor class]]) {
        // ARKit system plane anchor
        ARPlaneAnchor *updatedPlaneAnchor = (ARPlaneAnchor *)updatedAnchor;
        [self updatePlaneAnchorData:updatedPlaneAnchor toDictionary: anchorDictionary];
    } else if ([updatedAnchor isKindOfClass:[ARImageAnchor class]]) {
        // User generated ARImageAnchor, do nothing more than updating the transform
        return;
    } else if ([updatedAnchor isKindOfClass:[ARFaceAnchor class]]) {
        // System generated ARFaceAnchor
        ARFaceAnchor *faceAnchor = (ARFaceAnchor *)updatedAnchor;
        [self updateFaceAnchorData:faceAnchor toDictionary: anchorDictionary];
    } else {
        // Simple, user generated ARAnchor, do nothing more than updating the transform
        return;
    }
}

- (NSDictionary *)createDictionaryForAnchor:(ARAnchor *)addedAnchor {
    NSMutableDictionary* anchorDictionary = [NSMutableDictionary new];
    anchorDictionary[WEB_AR_TRANSFORM_OPTION] = arrayFromMatrix4x4([addedAnchor transform]);
    anchorDictionary[WEB_AR_MUST_SEND_OPTION] = [self shouldSendAnchor:addedAnchor] ? @(YES) : @(NO);
    
    if ([addedAnchor isKindOfClass:[ARPlaneAnchor class]]) {
        // ARKit system plane anchor
        ARPlaneAnchor *addedPlaneAnchor = (ARPlaneAnchor *)addedAnchor;
        [self addPlaneAnchorData:addedPlaneAnchor toDictionary: anchorDictionary];
        anchorDictionary[WEB_AR_UUID_OPTION] = [addedAnchor.identifier UUIDString];
        anchorDictionary[WEB_AR_ANCHOR_TYPE] = @"plane";
        DDLogDebug(@"Add Plane Anchor - %@", [addedAnchor.identifier UUIDString]);

    } else if ([addedAnchor isKindOfClass:[ARImageAnchor class]]) {
        // User generated ARImageAnchor
        ARImageAnchor *addedImageAnchor = (ARImageAnchor *)addedAnchor;
        arkitGeneratedAnchorIDUserAnchorIDMap[[[addedAnchor identifier] UUIDString]] = addedImageAnchor.referenceImage.name;
        anchorDictionary[WEB_AR_UUID_OPTION] = addedImageAnchor.referenceImage.name;
        anchorDictionary[WEB_AR_ANCHOR_TYPE] = @"image";
    } else if ([addedAnchor isKindOfClass:[ARFaceAnchor class]]) {
        // System generated ARFaceAnchor
        ARFaceAnchor *faceAnchor = (ARFaceAnchor *)addedAnchor;
        [self addFaceAnchorData:faceAnchor toDictionary: anchorDictionary];
        anchorDictionary[WEB_AR_UUID_OPTION] = [faceAnchor.identifier UUIDString];
        anchorDictionary[WEB_AR_ANCHOR_TYPE] = @"face";
    } else {
        // Simple, user generated ARAnchor
        NSString *userAnchorID = arkitGeneratedAnchorIDUserAnchorIDMap[[addedAnchor.identifier UUIDString]];
        NSString *name = userAnchorID? userAnchorID: [addedAnchor.identifier UUIDString];
        anchorDictionary[WEB_AR_UUID_OPTION] = name;
        anchorDictionary[WEB_AR_ANCHOR_TYPE] = @"anchor";
        DDLogDebug(@"Add User Anchor - %@", name);
    }

    return [anchorDictionary copy];
}

- (void)updateFaceAnchorData:(ARFaceAnchor *)faceAnchor toDictionary:(NSMutableDictionary *)faceAnchorDictionary {
    NSMutableDictionary *geometryDictionary = faceAnchorDictionary[WEB_AR_GEOMETRY_OPTION];
    if (!geometryDictionary) {
        geometryDictionary = [NSMutableDictionary new];
        faceAnchorDictionary[WEB_AR_GEOMETRY_OPTION] = geometryDictionary;
    }
    NSMutableArray* vertices = [NSMutableArray arrayWithCapacity:faceAnchor.geometry.vertexCount];
    for (int i = 0; i < faceAnchor.geometry.vertexCount; i++) {
        [vertices addObject:dictFromVector3(faceAnchor.geometry.vertices[i])];
    }
    geometryDictionary[@"vertices"] = vertices;

    NSMutableArray *blendShapesDictionary = faceAnchorDictionary[WEB_AR_BLEND_SHAPES_OPTION];
    [self setBlendShapes:faceAnchor.blendShapes toArray:blendShapesDictionary];

    // Remove the rest of the geometry data, since it doesn't change
    geometryDictionary[@"vertexCount"] = nil;
    geometryDictionary[@"textureCoordinateCount"] = nil;
    geometryDictionary[@"textureCoordinates"] = nil;
    geometryDictionary[@"triangleCount"] = nil;
    geometryDictionary[@"triangleIndices"] = nil;
}

- (void)addFaceAnchorData:(ARFaceAnchor *)faceAnchor toDictionary:(NSMutableDictionary *)faceAnchorDictionary {
    NSMutableArray *blendShapesArray = [NSMutableArray new];
    [self setBlendShapes:faceAnchor.blendShapes toArray:blendShapesArray];
    faceAnchorDictionary[WEB_AR_BLEND_SHAPES_OPTION] = blendShapesArray;

    NSMutableDictionary *geometryDictionary = [NSMutableDictionary new];
    [self addFaceGeometryData: faceAnchor.geometry toDictionary:geometryDictionary];
    faceAnchorDictionary[WEB_AR_GEOMETRY_OPTION] = geometryDictionary;
}

-(void)addFaceGeometryData:(ARFaceGeometry *)faceGeometry toDictionary: (NSMutableDictionary*)geometryDictionary {
    geometryDictionary[@"vertexCount"] = @(faceGeometry.vertexCount);

    NSMutableArray* vertices = [NSMutableArray arrayWithCapacity:faceGeometry.vertexCount];
    for (int i = 0; i < faceGeometry.vertexCount; i++) {
        [vertices addObject:dictFromVector3(faceGeometry.vertices[i])];
    }
    geometryDictionary[@"vertices"] = vertices;

    NSMutableArray* textureCoordinates = [NSMutableArray arrayWithCapacity:faceGeometry.textureCoordinateCount];
    geometryDictionary[@"textureCoordinateCount"] = @(faceGeometry.textureCoordinateCount);
    for (int i = 0; i < faceGeometry.textureCoordinateCount; i++) {
        [textureCoordinates addObject: dictFromVector2(faceGeometry.textureCoordinates[i])];
    }
    geometryDictionary[@"textureCoordinates"] = textureCoordinates;

    geometryDictionary[@"triangleCount"] = @(faceGeometry.triangleCount);

    NSMutableArray* triangleIndices = [NSMutableArray arrayWithCapacity:faceGeometry.triangleCount*3];
    for (int i = 0; i < faceGeometry.triangleCount*3; i++) {
        [triangleIndices addObject:@(faceGeometry.triangleIndices[i])];
    }
    geometryDictionary[@"triangleIndices"] = triangleIndices;
}

-(void)setBlendShapes:(NSDictionary<ARBlendShapeLocation, NSNumber*> *)blendShapes toArray:(NSMutableArray*)blendShapesArray {
    blendShapesArray[0] = blendShapes[ARBlendShapeLocationBrowDownLeft];
    blendShapesArray[1] = blendShapes[ARBlendShapeLocationBrowDownRight];
    blendShapesArray[2] = blendShapes[ARBlendShapeLocationBrowInnerUp];
    blendShapesArray[3] = blendShapes[ARBlendShapeLocationBrowOuterUpLeft];
    blendShapesArray[4] = blendShapes[ARBlendShapeLocationBrowOuterUpRight];
    blendShapesArray[5] = blendShapes[ARBlendShapeLocationCheekPuff];
    blendShapesArray[6] = blendShapes[ARBlendShapeLocationCheekSquintLeft];
    blendShapesArray[7] = blendShapes[ARBlendShapeLocationCheekSquintRight];
    blendShapesArray[8] = blendShapes[ARBlendShapeLocationEyeBlinkLeft];
    blendShapesArray[9] = blendShapes[ARBlendShapeLocationEyeBlinkRight];
    blendShapesArray[10] = blendShapes[ARBlendShapeLocationEyeLookDownLeft];
    blendShapesArray[11] = blendShapes[ARBlendShapeLocationEyeLookDownRight];
    blendShapesArray[12] = blendShapes[ARBlendShapeLocationEyeLookInLeft];
    blendShapesArray[13] = blendShapes[ARBlendShapeLocationEyeLookInRight];
    blendShapesArray[14] = blendShapes[ARBlendShapeLocationEyeLookOutLeft];
    blendShapesArray[15] = blendShapes[ARBlendShapeLocationEyeLookOutRight];
    blendShapesArray[16] = blendShapes[ARBlendShapeLocationEyeLookUpLeft];
    blendShapesArray[17] = blendShapes[ARBlendShapeLocationEyeLookUpRight];
    blendShapesArray[18] = blendShapes[ARBlendShapeLocationEyeSquintLeft];
    blendShapesArray[19] = blendShapes[ARBlendShapeLocationEyeSquintRight];
    blendShapesArray[20] = blendShapes[ARBlendShapeLocationEyeWideLeft];
    blendShapesArray[21] = blendShapes[ARBlendShapeLocationEyeWideRight];
    blendShapesArray[22] = blendShapes[ARBlendShapeLocationJawForward];
    blendShapesArray[23] = blendShapes[ARBlendShapeLocationJawLeft];
    blendShapesArray[24] = blendShapes[ARBlendShapeLocationJawOpen];
    blendShapesArray[25] = blendShapes[ARBlendShapeLocationJawRight];
    blendShapesArray[26] = blendShapes[ARBlendShapeLocationMouthClose];
    blendShapesArray[27] = blendShapes[ARBlendShapeLocationMouthDimpleLeft];
    blendShapesArray[28] = blendShapes[ARBlendShapeLocationMouthDimpleRight];
    blendShapesArray[29] = blendShapes[ARBlendShapeLocationMouthFrownLeft];
    blendShapesArray[30] = blendShapes[ARBlendShapeLocationMouthFrownRight];
    blendShapesArray[31] = blendShapes[ARBlendShapeLocationMouthFunnel];
    blendShapesArray[32] = blendShapes[ARBlendShapeLocationMouthLeft];
    blendShapesArray[33] = blendShapes[ARBlendShapeLocationMouthLowerDownLeft];
    blendShapesArray[34] = blendShapes[ARBlendShapeLocationMouthLowerDownRight];
    blendShapesArray[35] = blendShapes[ARBlendShapeLocationMouthPressLeft];
    blendShapesArray[36] = blendShapes[ARBlendShapeLocationMouthPressRight];
    blendShapesArray[37] = blendShapes[ARBlendShapeLocationMouthPucker];
    blendShapesArray[38] = blendShapes[ARBlendShapeLocationMouthRight];
    blendShapesArray[39] = blendShapes[ARBlendShapeLocationMouthRollLower];
    blendShapesArray[40] = blendShapes[ARBlendShapeLocationMouthRollUpper];
    blendShapesArray[41] = blendShapes[ARBlendShapeLocationMouthShrugLower];
    blendShapesArray[42] = blendShapes[ARBlendShapeLocationMouthShrugUpper];
    blendShapesArray[43] = blendShapes[ARBlendShapeLocationMouthSmileLeft];
    blendShapesArray[44] = blendShapes[ARBlendShapeLocationMouthSmileRight];
    blendShapesArray[45] = blendShapes[ARBlendShapeLocationMouthStretchLeft];
    blendShapesArray[46] = blendShapes[ARBlendShapeLocationMouthStretchRight];
    blendShapesArray[47] = blendShapes[ARBlendShapeLocationMouthUpperUpLeft];
    blendShapesArray[48] = blendShapes[ARBlendShapeLocationMouthUpperUpRight];
    blendShapesArray[49] = blendShapes[ARBlendShapeLocationNoseSneerLeft];
    blendShapesArray[50] = blendShapes[ARBlendShapeLocationNoseSneerRight];
}

-(void)updatePlaneGeometryData:(ARPlaneGeometry*)planeGeometry toDictionary:(NSMutableDictionary*)planeGeometryDictionary {
    planeGeometryDictionary[@"vertexCount"] = [NSNumber numberWithInteger:planeGeometry.vertexCount];

    NSMutableArray* vertices = [NSMutableArray arrayWithCapacity:planeGeometry.vertexCount];
    for (int i = 0; i < planeGeometry.vertexCount; i++) {
        [vertices addObject:dictFromVector3(planeGeometry.vertices[i])];
    }
    planeGeometryDictionary[@"vertices"] = vertices;

    NSMutableArray* textureCoordinates = [NSMutableArray arrayWithCapacity:planeGeometry.textureCoordinateCount];
    planeGeometryDictionary[@"textureCoordinateCount"] = [NSNumber numberWithInteger:planeGeometry.textureCoordinateCount];
    for (int i = 0; i < planeGeometry.textureCoordinateCount; i++) {
        [textureCoordinates addObject: dictFromVector2(planeGeometry.textureCoordinates[i])];
    }
    planeGeometryDictionary[@"textureCoordinates"] = textureCoordinates;

    planeGeometryDictionary[@"triangleCount"] = [NSNumber numberWithInteger:planeGeometry.triangleCount];

    NSMutableArray* triangleIndices = [NSMutableArray arrayWithCapacity:planeGeometry.triangleCount*3];
    for (int i = 0; i < planeGeometry.triangleCount*3; i++) {
        [triangleIndices addObject: [NSNumber numberWithInteger:planeGeometry.triangleIndices[i]]];
    }
    planeGeometryDictionary[@"triangleIndices"] = triangleIndices;

    planeGeometryDictionary[@"boundaryVertexCount"] = [NSNumber numberWithInteger:planeGeometry.boundaryVertexCount];

    NSMutableArray* boundaryVertices = [NSMutableArray arrayWithCapacity:planeGeometry.boundaryVertexCount];
    for (int i = 0; i < planeGeometry.boundaryVertexCount; i ++) {
        [boundaryVertices addObject: dictFromVector3(planeGeometry.boundaryVertices[i])];
    }
    planeGeometryDictionary[@"boundaryVertices"] = boundaryVertices;
}

-(void)addGeometryData:(ARPlaneGeometry*)planeGeometry toDictionary:(NSMutableDictionary*)dictionary {
    NSMutableDictionary* geometryDictionary = [NSMutableDictionary new];
    [self updatePlaneGeometryData:planeGeometry toDictionary:geometryDictionary];
    dictionary[WEB_AR_GEOMETRY_OPTION] = geometryDictionary;
}

- (void)addPlaneAnchorData:(ARPlaneAnchor *)planeAnchor toDictionary:(NSMutableDictionary *)dictionary {
    dictionary[WEB_AR_PLANE_CENTER_OPTION] = dictFromVector3([planeAnchor center]);
    dictionary[WEB_AR_PLANE_EXTENT_OPTION] = dictFromVector3([planeAnchor extent]);
    dictionary[WEB_AR_PLANE_ALIGNMENT_OPTION] = @([planeAnchor alignment]);
    [self addGeometryData:[planeAnchor geometry] toDictionary:dictionary];
}

- (void)updatePlaneAnchorData:(ARPlaneAnchor *)planeAnchor toDictionary:(NSMutableDictionary *)planeAnchorDictionary {
    planeAnchorDictionary[WEB_AR_PLANE_CENTER_OPTION] = dictFromVector3([planeAnchor center]);
    planeAnchorDictionary[WEB_AR_PLANE_EXTENT_OPTION] = dictFromVector3([planeAnchor extent]);
    planeAnchorDictionary[WEB_AR_PLANE_ALIGNMENT_OPTION] = @([planeAnchor alignment]);
    [self updatePlaneGeometryData:[planeAnchor geometry] toDictionary:planeAnchorDictionary[WEB_AR_GEOMETRY_OPTION]];
}

- (void)session:(ARSession *)session didUpdateAnchors:(NSArray<ARAnchor*>*)anchors
{
    //DDLogDebug(@"Update Anchors - %@", [anchors debugDescription]);
    //DDLogDebug(@"Update Anchors - %lu", anchors.count);
    for (ARAnchor* updatedAnchor in anchors) {
        if ([updatedAnchor isKindOfClass:[ARFaceAnchor class]] && ![self.configuration isKindOfClass:[ARFaceTrackingConfiguration class]]) {
            NSLog(@"Trying to update a face anchor in a session configuration that's not ARFaceTrackingConfiguration");
            continue;
        }
        
        [self updateDictionaryForAnchor:updatedAnchor];
    }
}

- (void)session:(ARSession *)session didRemoveAnchors:(NSArray<ARAnchor*>*)anchors
{
    DDLogDebug(@"Remove Anchors - %@", [anchors debugDescription]);
    for (ARAnchor* removedAnchor in anchors) {
        
        // logic makes no sense:  if the anchor is in objects[] list, remove it and send removed flag.  otherwise, ignore
        //        BOOL mustSendAnchor = [self shouldSendAnchor: removedAnchor];
        //        if (mustSendAnchor ||
        //            self.sendingWorldSensingDataAuthorizationStatus == SendWorldSensingDataAuthorizationStateAuthorized) {
        NSString* anchorID = [self anchorIDForAnchor: removedAnchor];
        if (objects[anchorID]) {
            [removedAnchorsSinceLastFrame addObject: anchorID];
            objects[anchorID] = nil;

            arkitGeneratedAnchorIDUserAnchorIDMap[anchorID] = nil;
            if ([removedAnchor isKindOfClass:[ARImageAnchor class]]) {
                ARImageAnchor* imageAnchor = (ARImageAnchor*)removedAnchor;
                ActivateDetectionImageCompletionBlock completion = self.detectionImageActivationAfterRemovalPromises[imageAnchor.referenceImage.name];
                if (completion) {
                    [self activateDetectionImage:imageAnchor.referenceImage.name completion:completion];
                    self.detectionImageActivationAfterRemovalPromises[imageAnchor.referenceImage.name] = nil;
                }
            }
        }
    }

    // Inform up in the calling hierarchy when we have plane anchors removed from the scene
    if ([self didRemovePlaneAnchors]) {
        if ([self anyPlaneAnchor:anchors]) {
            [self didRemovePlaneAnchors]();
        }
    }
}

- (NSString *)anchorIDForAnchor:(ARAnchor *)anchor {
    NSString* anchorID;
    if ([anchor isKindOfClass:[ARPlaneAnchor class]]) {
        // ARKit system plane anchor
        anchorID = [anchor.identifier UUIDString];
    } else if ([anchor isKindOfClass:[ARImageAnchor class]]) {
        // User generated ARImageAnchor
        ARImageAnchor *imageAnchor = (ARImageAnchor *)anchor;
        anchorID = imageAnchor.referenceImage.name;
    } else if ([anchor isKindOfClass:[ARFaceAnchor class]]) {
        // System generated ARFaceAnchor
        anchorID = [anchor.identifier UUIDString];
    } else {
        // Simple, user generated ARAnchor
        NSString *userAnchorID = arkitGeneratedAnchorIDUserAnchorIDMap[[anchor.identifier UUIDString]];
//        NSString *anchorName = anchor.name;
//        NSString *name;
//        if (userAnchorID) {
//            name = userAnchorID;
//        } else {
//            name = [anchor.identifier UUIDString];
//        }
        NSString *name = userAnchorID? userAnchorID: [anchor.identifier UUIDString];
        anchorID = name;
    }

    return anchorID;
}

/**
 By default, set NO to all the ARAnchor types considered "World sensing data", so they
 won't be sent to JS unless the user allows for that.

 @param anchor The anchor to be analyzed
 @return A boolean indicating whether the anchor should be sent to JS or not
 */
- (BOOL)shouldSendAnchor:(ARAnchor *)anchor {
    BOOL shouldSend;
    if ([anchor isKindOfClass:[ARPlaneAnchor class]]) {
        // ARKit system plane anchor
#if SEND_PLANES_BY_DEFAULT
        shouldSend = YES;
#else
        shouldSend = NO;
#endif
    } else if ([anchor isKindOfClass:[ARImageAnchor class]]) {
        // User generated ARImageAnchor
        shouldSend = NO;
    } else if ([anchor isKindOfClass:[ARFaceAnchor class]]) {
        shouldSend = NO;
        // System generated ARFaceAnchor
    } else {
        // Simple, user generated ARAnchor
        shouldSend = YES;
    }

    return shouldSend;
}

- (BOOL)anyPlaneAnchor:(NSArray<ARAnchor *> *)anchorArray {
    BOOL anyPlaneAnchor = NO;
    for (ARAnchor *anchor in anchorArray) {
        if ([anchor isKindOfClass:[ARPlaneAnchor class]]) {
            anyPlaneAnchor = YES;
            break;
        }
    }
    return anyPlaneAnchor;
}

#pragma mark ARSessionObserver

- (void)session:(ARSession *)session didFailWithError:(NSError *)error
{
    DDLogError(@"Session didFailWithError - %@", error);
    
    [self setArSessionState:ARKSessionUnknown];
    
    if ([self didFailSession])
    {
        [self didFailSession](error);
    }
}

- (void)session:(ARSession *)session cameraDidChangeTrackingState:(ARCamera *)camera
{
    DDLogDebug(@"Session cameraDidChangeTrackingState - %@", trackingState(camera));
    
    if ([self didChangeTrackingState])
    {
        [self didChangeTrackingState](trackingState(camera));
    }
    
    [[self controller] didChangeTrackingState:camera];
}

- (void)sessionWasInterrupted:(ARSession *)session
{
    DDLogError(@"Session WasInterrupted");
    
    if ([self didInterrupt])
    {
        [self didInterrupt](YES);
    }
}

- (void)sessionInterruptionEnded:(ARSession *)session
{
    DDLogError(@"Session InterruptionEnded");
    
    if ([self didInterrupt])
    {
        [self didInterrupt](NO);
    }
}

- (BOOL)sessionShouldAttemptRelocalization:(ARSession *)session
{
    return YES;
}

- (void)session:(ARSession *)session didOutputAudioSampleBuffer:(CMSampleBufferRef)audioSampleBuffer
{
    //DDLogDebug(@"Session didOutputAudioSampleBuffer");
}

@end

